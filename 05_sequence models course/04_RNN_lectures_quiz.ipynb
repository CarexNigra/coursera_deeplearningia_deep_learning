{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccaa843b",
   "metadata": {},
   "source": [
    "# Week 4: Transformers\n",
    "\n",
    "### (video 1) Transformer network\n",
    "\n",
    "* Many of the most effective algorithms for NLP today are based on the transformer architecture\n",
    "* As the complexity of your sequence task increases, so does the complexity of your model.\n",
    "    * RNN: \n",
    "        * vanishing / exploding gradients =>\n",
    "        * hard to capture long range dependencies and sequences\n",
    "    * GRU + LSTM:\n",
    "        * resolve many of those problems: gates used to control the flow of information.\n",
    "        * bottleneck to the flow of information: to compute the output of this final unit, for example, you first have to compute the outputs of all of the units that come before\n",
    "    * transformer architecture, \n",
    "        * allows you to run a lot more of these computations for an entire sequence in parallel.\n",
    "        * ingest an entire sentence all at the same time, rather than just processing it one word at a time from left to right\n",
    "        \n",
    "<img src='./Images/W4_01.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**Transformer network intuition**\n",
    "\n",
    "* RNN\n",
    "    * processes 1 output at a time\n",
    "\n",
    "* The major innovation of the transformer architecture is combining the use of \n",
    "    * attention based representations = a way of computing very rich, very useful representations of words\n",
    "    * and a CNN convolutional neural network style of processing.\n",
    "\n",
    "<img src='./Images/W4_02.png' style=\"width: 60%\"></img>\n",
    "\n",
    "2 key ideas:\n",
    "* Reminder: Attention mechannism\n",
    "    * allows output to focus attention on input while producing output\n",
    "* (1) Self-attention:\n",
    "    * allows inputs to interact with each other\n",
    "    * if you have a sentence of five words \n",
    "    * you will end up computing five representations for these five words (A1,A2,A3,A4,A5).\n",
    "    * And this will be an attention based way of computing representations for all the words in your sentence in parallel. \n",
    "* (2) Multi-headed attention: \n",
    "    * is basically a \"for\" loop over the self attention process.\n",
    "    * You end up with multiple versions of these representations (very rich)\n",
    "    * these representations can be used for machine translation or other NLP tasks to create effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d117bb3d",
   "metadata": {},
   "source": [
    "### (video 2) Self-attention\n",
    "\n",
    "* You've seen how attention is used with sequential neural networks such as RNNs. \n",
    "* To use attention with a style more late CNNs, you need to calculate self-attention\n",
    "    * where you create attention-based representations for each of the words in your input sentence. \n",
    "    \n",
    "* Jane, visite, l'Afrique, en, septembre, \n",
    "    * our goal will be for each word to compute an attention-based representation (A1,A2,A3,A4,A5)   \n",
    "\n",
    "* Take the word l'Afrique => A3 representation\n",
    "    * One way to represent l'Afrique would be to just look up the word embedding for l'Afrique.\n",
    "    * But depending on the context, are we thinking of l'Afrique as of a \n",
    "        * site of historical interests \n",
    "        * as a holiday destination\n",
    "        * or as the world's second largest continent\n",
    "    * Depending on context, you may choose to represent it differently\n",
    "        * A3 will look into sourrounding words, gets the context and gives the best representation\n",
    "    * It won't be too different from the attention mechanism you saw in the context of RNNs,\n",
    "        * except we'll compute these representations in parallel for all five words in a sentence\n",
    "        * the equasions are quite similar\n",
    "            * also involves a softmax\n",
    "            * exponent terms are akin to attention values\n",
    "        * the main difference is that for every word you have 3 values\n",
    "            * query = q3, \n",
    "            * key = k3,\n",
    "            * value = v3\n",
    "            * they are the vector inputs to computing attention value for each word \n",
    "        \n",
    "<img src='./Images/W4_04.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**Computations:** \n",
    "\n",
    "(1) We are going to associate each of the words with 3 values: q, k, v\n",
    "* q3 = Wq * x3  \n",
    "* k3 = Wk * x3\n",
    "* v3 = Wv * x3\n",
    "\n",
    "Wq, Wk, Wv = matrices of parameters, they allow you to pull off these query, key, and value vectors for each word\n",
    "\n",
    "(2) Compute how likely each word is an answer to a certain question about a selected word\n",
    "* q3 = the question you are going to ask about the word 3: eg: what's happening therу?\n",
    "    * we compute the inner product between q^3 and k^1,\n",
    "    * this will tell us how good `Word 1 = Jane` answers the question of what's happening in Africa\n",
    "    * we compute the inner product between q^3 and k^2,\n",
    "    * how good `Word 2 = visite` answers the question of what's happening in Africa\n",
    "\n",
    "The goal of this operation is to pull up the most information that's needed to help us compute the most useful representation A^3\n",
    "\n",
    "For intuition:\n",
    "* if k^1 represents that this word is a person,\n",
    "* and k^2 represents that the second word, visite, is an action, \n",
    "* then you may find that q^3 inter producted with k^2 has the largest value, \n",
    "* and this may be intuitive example, might suggest that visite, gives you the most relevant contexts for what's happening in Africa\n",
    "\n",
    "(3) Then we take all dot products for selected word and compute a soft max over them (exp(q * k) / sum(exp(q * k)) \n",
    "\n",
    "(4) Then we're going to take these Softmax values and multiply them with v^1, which is the value for word 1, the value for word 2, and so on (v)\n",
    "\n",
    "(5) Finally, we sum it all up\n",
    "\n",
    "**The key advantage** of this representation is that\n",
    "* the word of l'Afrique isn't some fixed word embedding. \n",
    "* Instead, it lets the self-attention mechanism realize that l'Afrique is the destination of a visite\n",
    "* and thus compute a richer, more useful representation for this word\n",
    "\n",
    "(6) YOu do this for all words in the sequence and summarize this in one single attention, where\n",
    "* another name for this type of attention is the \"scaled dot-product attention\"\n",
    "* Q, K, V are matrices of all these qi, ki, vi values for all words\n",
    "* on the right under softmax = vectorized representation of the equation with exponents\n",
    "    * denominator = just to scale the dot-product, so it doesn't explode.\n",
    "\n",
    "<img src='./Images/W4_05.png' style=\"width: 80%\"></img>\n",
    "\n",
    "**Recap:**\n",
    "* associated with each of the five words you end up with a query, a key, and a value. \n",
    "    * The query lets you ask a question about that word, such as what's happening in Africa.\n",
    "        * Q = interesting questions about the words in a sentence,  \n",
    "    * The key \n",
    "        * K = qualities of words given a Q, \n",
    "        * looks at all of the other words, \n",
    "        * and by the similarity to the query, helps you figure out which words gives the most relevant answer to that question. \n",
    "        * In this case, visite is what's happening in Africa, someone's visiting Africa. \n",
    "    * The value \n",
    "        * V = specific representations of words given a Q\n",
    "        * allows the representation to plug in how visite should be represented within A^3, within the representation of Africa. \n",
    "* This allows you to come up with a representation for the word Africa that says this is Africa and someone is visiting Africa.\n",
    "* This is a much richer representation for the world than \n",
    "    * if you just had to pull up the same fixed word embedding for every single word \n",
    "    * without being able to adapt it based on what words are to the left and to the right of that word. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8c92c",
   "metadata": {},
   "source": [
    "### (video 3) Multi-headed attention\n",
    "\n",
    "* it is basically just a big four loop over the self attention mechanism \n",
    "* each time you calculate self attention for a sequence is called a head\n",
    "* thus the name multi head attention refers to if you do what you saw in the last video, but a bunch of times \n",
    "\n",
    "* With simple self-attention: \n",
    "    * you got the vectors Q K and V for each of the input terms by \n",
    "    * multiplying them (terms) by a few matrices, Wq Wk and Wv weight matricies \n",
    "\n",
    "*  With multi head attention for **each word**\n",
    "    * you take that same set of query key and value vectors as inputs (Q K and V)\n",
    "    * and calculate multiple self attentionsby multiplying them with weight matrices,\n",
    "        * w_1_q, w_1_k, w_1_v => \n",
    "        * resulting values give a new set of q, k and v values for the word\n",
    "        * then do the same for all other words (Ws are the same)\n",
    "        * w_1_q, w_1_k, w_1_v = are wights to be learnt to help asking and anwering the question\n",
    "    * We do this with second head\n",
    "        *  w_2_q, w_2_k, w_2_v => allows to answer the second question: when is smth happening\n",
    "        * september = good answer to the question (for the word Afrique)\n",
    "    * etc\n",
    "       \n",
    "* h = number of heads (parameter) = features\n",
    "    * the concatination of these values are concatinated and to compute the output of the multi headed attention \n",
    "    \n",
    "* Final value: concatination of these heads, multiplied by matrix W\n",
    "\n",
    "* I described this as a \"for loop\", but you can actually compute these different heads' values in parallel because no one has value depends on the value of any other head\n",
    "        \n",
    "\n",
    "<img src='./Images/W4_06.png' style=\"width: 60%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3b962",
   "metadata": {},
   "source": [
    "### (video 4) Transformer network\n",
    "\n",
    "* Same task: translation of the sequence from french to english \n",
    "* Up until this point, for the sake of simplicity, \n",
    "    * I've only been talking about the embeddings for the words in the sentence. \n",
    "    * But in many sequences sequence translation task, will be useful to also add\n",
    "        * the start of sentence = SOS token\n",
    "        * and the end of sentence = EOS tokens\n",
    "\n",
    "(1) The first step these embeddings get fed into an encoder block\n",
    "* (a) which has a multi head attention there. \n",
    "    * you feed in the values Q K and V computed from \n",
    "        * the embeddings \n",
    "        * and the weight matrices W. \n",
    "    * This layer then produces a matrix that can be \n",
    "* (b) passed into a feed forward neural network.\n",
    "    * Which helps determine what interesting features there are in the sentence. \n",
    "In the transformer paper, this block, this encoding block is repeated N times and a typical value for N is six.\n",
    "\n",
    "(2) After these 6 times in encoder, the result is fed to the decoder block\n",
    "* First output is the SOS token\n",
    "* and it takes as an input the encoders input + everything already produced previously by decoder \n",
    "\n",
    "* (a) So SOS token is passed the first and is used to compute Q K and V for this first multi-headed attention block\n",
    "    * This first blocks, output is used to generate the Q matrix for the next multi head attention block = what you've been translated so far\n",
    "    * And the output of the **encoder** is used to generate K and V = give context\n",
    "* (b) second multi-head attention \n",
    "    * Calculates attention \n",
    "* (c) the output transfered to the FF neural network that predicts the next word\n",
    "    * tries to decide which is the next word to generate\n",
    "    * The SOS + next word = input to the decoder\n",
    "\n",
    "The decoder is also repeated N times\n",
    "\n",
    "<img src='./Images/W4_07.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**Beyond these main ideas, there are a few extra bells and whistles:**\n",
    "* (1) Positional encoding of the input:\n",
    "    * the way you encode the position of elements in the input is that you use a combination of these sine and cosine equations\n",
    "    *  let's say for example that your word embedding is a vector with four values. In this case the dimension D of the word embedding is 4.\n",
    "    *  we're going to then create a positional embedded in vector of the same dimension\n",
    "        * In this equation below, \n",
    "        * pos = denotes the numerical position of the word. \n",
    "        * So for the word Jane, pos is equal to 1 \n",
    "        * and i = refers to the different dimensions of the encoding vector\n",
    "        * The first element responds to I equals 0. \n",
    "        * This element i equals 1,i equals 2,i equals 3.\n",
    "        * d = 4 = dimention of enoding vector\n",
    "        * sine/cosine used to create unique position encoding vector for each word\n",
    "        * equation => values in each cell of the position vector\n",
    "        * you read values from d sine/cosine graphs for each consecutive word and fill vector with them\n",
    "    * Positional encoding of P1 is added directly to X1\n",
    "    \n",
    "=> The output of the encoding block contains contextual semantic embedding and positional encoding information. \n",
    "\n",
    "The output of the embedding layer is of dimension:\n",
    "* d = in this case 4 \n",
    "* by the maximum length of sequence your model can take. \n",
    "\n",
    "The outputs of all other layers of the encoder are also of this shape.\n",
    "\n",
    "<img src='./Images/W4_08.png' style=\"width: 30%\"></img>\n",
    "\n",
    "* (2) Residual connections: Add and Norm\n",
    "    * In addition to adding these position encodings to the embeddings, you'd also pass them through the network with residual connections. \n",
    "    * The purposein this case: to pass along positional information through the entire architecture.\n",
    "    * Similar to bash norm \n",
    "    * This helps to speed up learning (**question** by normalizing the length of sentences?)\n",
    "    * repeated in several places\n",
    "   \n",
    "* (3) Output of the decoder block is linearized and a soft max applied\n",
    "\n",
    "\n",
    "* (4) You may also to hear of Masked Multi-head attention for first layer of the decoder\n",
    "    * It is important for the trainig set only\n",
    "    * Let's say your data set has the correct french to English translation\n",
    "    * When training you have access to the entire correct English translation, the correct output and they're correct input.\n",
    "    * And because you have the full correct output you don't actually have to generate the words one at a time during training.\n",
    "    * masking blocks out the last part of the sentence to mimic what the network will need to do at test time or during prediction\n",
    "    \n",
    "    \n",
    "<img src='./Images/W4_09.png' style=\"width: 80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1f673",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "<img src='./Images/Q4_1.png' style=\"width: 80%\"></img>\n",
    "\n",
    "I don't understand what are \"word values\" in the 3d exercise\n",
    "\n",
    "<img src='./Images/Q4_2.png' style=\"width: 80%\"></img>\n",
    "<img src='./Images/Q4_3.png' style=\"width: 80%\"></img>\n",
    "<img src='./Images/Q4_4.png' style=\"width: 80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700853b",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "**Week 1:**\n",
    "* Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy (GitHub: karpathy)\n",
    "* The Unreasonable Effectiveness of Recurrent Neural Networks (Andrej Karpathy blog, 2015)\n",
    "* deepjazz (GitHub: jisungk)\n",
    "* Learning Jazz Grammars (Gillick, Tang & Keller, 2010)\n",
    "* A Grammatical Approach to Automatic Improvisation (Keller & Morrison, 2007)\n",
    "* Surprising Harmonies (Pachet, 1999)\n",
    "\n",
    "**Week 2:**\n",
    "* Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings (Bolukbasi, Chang, Zou, Saligrama​ & Kalai, 2016)\n",
    "* GloVe: Global Vectors for Word Representation (Pennington, Socher & Manning, 2014)\n",
    "* Woebot.\n",
    "\n",
    "**Week 4:**\n",
    "* Natural Language Processing Specialization (by DeepLearning.AI)\n",
    "* Attention Is All You Need (Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser​ & Polosukhin, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b7ec1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32f4e8e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a28bc44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "396bdb6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d4fce6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41c07458",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbe3c36e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a8128ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "985c31b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a7c959c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "220f0c9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5e20e7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e364386e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caa8e118",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acc963a2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
