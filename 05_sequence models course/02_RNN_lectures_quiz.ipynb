{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3479f5f3",
   "metadata": {},
   "source": [
    "# Week 2: NLP and Word Embeddings\n",
    "\n",
    "Intro to word embeddings\n",
    "\n",
    "### (video 1) Word Representation\n",
    "\n",
    "**Word embeddings** – the way to represent words that algorithm understands analogies\n",
    "* Through these ideas of word embeddings, you'll be able to build NPL applications, even with relatively small labeled training sets.\n",
    "* you'll see how to debias word embeddings to reduce undesirable gender or ethnicity or other types of bias that learning algorithms can sometimes pick up\n",
    "\n",
    "Words represented with one-hot vectors o<sub>word_index</sub>\n",
    "\n",
    "<img src='./Images/W2_01.png' style=\"width: 60%\"></img>\n",
    "\n",
    "One of the **weaknesses** of this representation is that it treats each word as a thing unto itself, and it doesn't allow an algorithm to easily generalize the cross words\n",
    "\n",
    "**Example** \n",
    "* let's say you have a language model that has learned that when you see \n",
    "    * I want a glass of orange ____.\n",
    "    * the next word will be very likely *juice*. \n",
    "* But even if the learning algorithm has learned that it is a likely sentence, if it sees \n",
    "    * I want a glass of apple ____. \n",
    "* As far as it knows the relationship between apple and orange is *not any closer* as the relationship between any of the other words\n",
    "* this is because the any product between **any two different one-hot vector is zero** => the distance between any two vectors is the same\n",
    "\n",
    "**Featurized representation: word embedding**\n",
    "\n",
    "<img src='./Images/W2_02.png' style=\"width: 60%\"></img>\n",
    "\n",
    "* 300 different features => 300 dim vector representing the word \"man\" = e<sub>word_index</sub>\n",
    "* If you use this representation to represent the words orange and apple, then notice that the representations for orange and apple are now quite similar.\n",
    "* But features won't have an evident interpretation (component one = gender, component two = royal, ...). Exactly what they're representing will be harder to figure out\n",
    "* Popular thing to do is to embed 300 dim word representation in 2D space to visualize them. Common algorithm to do that is t-SNE algorithm (non-linear dimensionality reduction technique). These representations are called **embeddings**\n",
    "\n",
    "<img src='./Images/W2_03.png' style=\"width: 60%\"></img>\n",
    "        \n",
    "\n",
    "### (video 2) Using Word Embeddings\n",
    "\n",
    "<img src='./Images/W2_04.png' style=\"width: 60%\"></img>\n",
    "\n",
    "* You'll figure out that Sally Johnson is a person's name, hence, the outputs 1 like that. \n",
    "    * And one way to be sure that Sally Johnson has to be a person, rather than say the name of the corporation is that you know orange farmer is a person\n",
    "    * Knowing that orange and apple are very similar will make it easier for your learning algorithm to generalize to figure out that Robert Lin is also a human, is also a person's name\n",
    "    * if you see less common wirds: Robert Lin is a durian cultivator? \n",
    "        * Those words could be absent in your training set\n",
    "        * But if they are generalized with more common within voacb, they'll be recognized\n",
    "    * the algorithms to learning word embeddings can examine very large text corpuses => **transfer learning**\n",
    "* One nice thing\n",
    "    * ou can now use relatively lower dimensional feature vectors. \n",
    "    * Rather than using a 10,000 dimensional one-hot vector, you can now instead use a 300 dimensional dense vector. \n",
    "    * Although the one-hot vector is fast and the 300 dimensional vector that you might learn for your embedding will be a dense vector.\n",
    "\n",
    "<img src='./Images/W2_05.png' style=\"width: 60%\"></img>\n",
    "        \n",
    "* Useful for standard NLP tasks:\n",
    "    * named entity recognition, \n",
    "    * text summarization, \n",
    "    * co-reference resolution\n",
    "    * parsing\n",
    "* Less useful for language modeling, machine translation, etc\n",
    "\n",
    "<img src='./Images/W2_06.png' style=\"width: 60%\"></img>\n",
    "\n",
    "Word embeddings has relationship to the face encoding ideas that you learned about in convolutional neural networks course \n",
    "* we train ta network architecture that would learn, say, a 128 dimensional representation for different faces\n",
    "* One difference between the face recognition literature and what we do in word embeddings is that, for face recognition, you wanted to train a neural network that can take as input any face picture, even a picture you've never seen before, and have a neural network compute an encoding for that new picture.\n",
    "* whereas what we'll do for learning word embeddings is that we'll have a fixed vocabulary of, say, 10,000 words. And we'll learn a vector e<sub>1</sub> through, say, e<sub>10000</sub> that just learns a fixed embedding for each of the words in our vocabulary\n",
    "\n",
    "### (video 3) Properties of Word Embeddings\n",
    "\n",
    "<img src='./Images/W2_07.png' style=\"width: 60%\"></img>    \n",
    "\n",
    "* Question: man is to woman as king is to what?\n",
    "    * is it possible to have an algorithm figure this out automatically?\n",
    "    * e<sub>man</sub> - e<sub>woman</sub> = [-2, 0, ..., 0]\n",
    "    * e<sub>king</sub> - e<sub>queen</sub> = [-2, 0, ..., 0]\n",
    "    * => the main difference in both cases is gender\n",
    "    * we need to find a vector that makes the  two vectore diffs almost equal\n",
    "    * Actually those vect diffs represent the diff in gender\n",
    "    * We need a function that maximozes similarity between e<sub>king</sub> - e<sub>woman</sub> + e<sub>man</sub> and e<sub>w</sub>\n",
    "    * it's not uncommon for research papers to report anywhere from, say, 30% to 75% accuracy on analogy using tasks like these\n",
    "    \n",
    "<img src='./Images/W2_08.png' style=\"width: 60%\"></img> \n",
    "\n",
    "* t-SNE takes 300-D data, and it maps it in a very non-linear way to a 2D space\n",
    "* And there is no parallelism of vectrs in 2D space, only in 300D\n",
    "\n",
    "**Similarity function**\n",
    "* So the most commonly used similarity function is called **cosine similarity**.\n",
    "* you define the similarity between two vectors u and v as u transpose v divided by the Euclidean lengths    \n",
    "*  So ignoring the denominator for now, (u transpose v) is basically the inner product between u and v. \n",
    "    * And so if u and v are very similar, their inner product will tend to be large. \n",
    "    * And this is called cosine similarity because this is actually the cosine of the angle between the two vectors, u and v. \n",
    "    * So that's the angle phi, so this formula is actually the cosine between them. \n",
    "    * And so you remember from calculus that if this phi, then the cosine of phi looks like this. \n",
    "    * So if the angle between them is 0, then the cosine similarity is equal to 1. \n",
    "    * And if their angle is 90 degrees, the cosine similarity is 0. \n",
    "    * And then if they're 180 degrees, or pointing in completely opposite directions, it ends up being -1.    \n",
    "\n",
    "<img src='./Images/W2_09.png' style=\"width: 60%\"></img> \n",
    "\n",
    "\n",
    "* Anoher option is to use **square distance or Euclidian distance**: ||u-v||</sup>2</sup> \n",
    "    * Technically, this would be a measure of dissimilarity rather than a measure of similarity. \n",
    "    * So we need to take the negative of this, and this will work okay as well. \n",
    "    * Although I see cosine similarity being used a bit more often.\n",
    "    * the main difference between these is how it normalizes the lengths of the vectors u and v\n",
    "    \n",
    "    \n",
    "### (video 4) Embedding Matrix\n",
    "\n",
    "Notation:\n",
    "* E = embedding matrix of 10000 words x 300 properties\n",
    "* orange \n",
    "    * o<sub>6257</sub> one hot encoded. Vector dim 10000\n",
    "    * E * o<sub>6257</sub> = e<sub>6257</sub> Vector dim 300 \n",
    "    \n",
    "<img src='./Images/W2_10.png' style=\"width: 60%\"></img> \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc144649",
   "metadata": {},
   "source": [
    "Learning Word embeddigs; word2vect & GloVe\n",
    "\n",
    "### (video 5) Embedding Matrix\n",
    "\n",
    "**How to create**\n",
    "[petuum.medium.com](https://petuum.medium.com/embeddings-a-matrix-of-meaning-4de877c9aa27)\n",
    "[towardsdatascience.com](https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8)\n",
    "\n",
    "\n",
    "* Let's say you're building a language model and you do it with a neural network.\n",
    "    * Get all e<sub>index</sub> \n",
    "    * => transform to input layer: a 1,800 dimensional vector obtained by taking your six embedding vectors and stacking them together. \n",
    "    * => feed to NN \n",
    "    * => feed to softmax\n",
    "        * softmax classifies among the 10,000 possible outputs in the vocab for those final word we're trying to predict.\n",
    "* what's more commonly done is to have a **fixed historical window**. \n",
    "    * you might decide that you always want to predict the next word given say the previous 4 words, \n",
    "    * 4 = is a hyperparameter of the algorithm\n",
    "    * => input layer becomes 1200 dim\n",
    "    * using a fixed history, just means that you can deal with even arbitrarily long sentences\n",
    "\n",
    "<img src='./Images/W2_11.png' style=\"width: 60%\"></img> \n",
    "\n",
    "* Parameters of the algorithm\n",
    "    * E \n",
    "    * historical window\n",
    "    * weights and biases of NN\n",
    "    * weight and bias of softmax\n",
    "    \n",
    "NB: you can use that backprop to perform gradient decsent to maximize the likelihood of your training set to just repeatedly predict given four words in a sequence, what is the next word in your text corpus\n",
    "\n",
    "this is one of the earlier and pretty successful algorithms for learning word embeddings\n",
    "\n",
    "**Let's generalise**\n",
    "\n",
    "* We can select different contexts to predict target word \n",
    "\n",
    "<img src='./Images/W2_12.png' style=\"width: 60%\"></img> \n",
    "\n",
    "\n",
    "### (video 6) Word2Vec\n",
    "\n",
    "* Word2Vec algorithm which is simple and comfortably more efficient way to learn this types of embeddings.\n",
    "* In the skip-gram model we're going to come up with a few context to target words to create our supervised learning problem\n",
    "    * randomly pick a word to be the context word. And let's say we chose the word orange.\n",
    "    * we'll set up a supervised learning problem where given the context word,\n",
    "        * you're asked to predict what is a randomly chosen word within a certain word window of that input context word.\n",
    "    * we want to use this learning problem to learn good word embeddings\n",
    "\n",
    "<img src='./Images/W2_13.png' style=\"width: 60%\"></img> \n",
    "\n",
    "* we're going to learn the mapping \n",
    "    * from some Context c, such as the word orange \n",
    "    * to some target, which we will call t, which might be the word juice or the word glass or the word my\n",
    "\n",
    "<img src='./Images/W2_14.png' style=\"width: 60%\"></img> \n",
    "\n",
    "* Theta<sub>t</sub> – parameter associated with output t. What's the chance of a particular word t to be a label\n",
    "* Bias term is left off, but can be included\n",
    "* the loss function for softmax will be the usual.\n",
    "    * E matrix has a lot of parameters corresponding to all these embedding vectors e<sub>c</sub> \n",
    "    * softmax unit also has parameters that gives the Theta<sub>t</sub>\n",
    "\n",
    "**Problems with the algorithm**\n",
    "* (1) Computationally very expensive = every time you want to evaluate this probability, you need to carry out a sum over all 10,000 words in your vocabulary.\n",
    "    * Several solutions. One is to use a hierarchical softmax classifier\n",
    "        * instead of trying to categorize something into all 10,000 carries on one go\n",
    "        * Recursion: tell whether t in first part of vocab or second \n",
    "        * the o(classifier) log|vocab size| rather than linear\n",
    "        * in practice, the hierarchical softmax classifier doesn't use a perfectly balanced tree. Rather: сщmmon words on top, uncommon – deeper\n",
    "        \n",
    "<img src='./Images/W2_15.png' style=\"width: 60%\"></img>         \n",
    "        \n",
    "* (2) How to sample the context c?\n",
    "    * some usual words like \"and\", \"or\", \"the\" will fall into the samples frequently\n",
    "    * whereas others – mhaving more sense – rarely\n",
    "    * You don't want to be your training set domiated by first ones\n",
    "    * So in practice the distribution of words P(c) isn't taken just entirely uniformly at random for the training set purpose, \n",
    "        * but instead there are different heuristics that you could use in order to balance out something from the common words together with the less common words.\n",
    "        \n",
    "**Another algorithm: CBow**: the continuous backwards model, which takes the surrounding contexts from middle word, and uses the surrounding words to try to predict the middle word\n",
    "\n",
    "### (video 7) Negative Sampling\n",
    "\n",
    "Covers modified learning problem called negative sampling that allows you to do something similar to the Skip-Gram model, but with a much more efficient learning algorithm.\n",
    "\n",
    "**Generate training data set**\n",
    "\n",
    "<img src='./Images/W2_16.png' style=\"width: 60%\"></img>    \n",
    "\n",
    "* New supervised learning problem. \n",
    "    * Given a pair of words like orange and juice,\n",
    "    * We're going to predict, is this a context-target pair? \n",
    "* One positive exampele are generated exactly the way covered in the previous section\n",
    "    * Sample a context word, \n",
    "    * look around a window of say, plus-minus ten words and pick a target word. \n",
    "    * So that's how you generate the first row of this table with orange, juice, \n",
    "* To generate a set of negative examples for a certain number of time, \n",
    "    * you're going to take the same context word \n",
    "    * and then just pick a word at random from the dictionary\n",
    "    * So in this case, I chose the word king at random and we will label that as 0.\n",
    "    * and it's okay if just by chance, one of those words we picked at random from the dictionary happens to appear in the plus-minus X word window, next to the context word\n",
    "    \n",
    "* we're going to create a supervised learning problem \n",
    "    * where the learning algorithm inputs x = [pair of words: context, target],\n",
    "    * and it has to predict the target label to predict the output y.\n",
    "* This is how you generate the training set\n",
    "\n",
    "How to chose k?\n",
    "* k = 5-20 for smaller data sets\n",
    "* k = 2-5 for larger data sets\n",
    "\n",
    "**Supervised learning model for learning a mapping from x to y**\n",
    "\n",
    "<img src='./Images/W2_17.png' style=\"width: 60%\"></img> \n",
    "\n",
    "* So what we're going to do is define a logistic regression model. \n",
    "    * Say, that the chance of y = 1, given the input c, t pair\n",
    "    * one parameter vector theta for each possible target word. \n",
    "    * And a separate parameter vector, really the embedding vector, for each possible context word.\n",
    "    \n",
    "* Draw this as a NN:\n",
    "    * 10k logistic regressions\n",
    "        * Where one of these will be the classifier corresponding to: is the target word juice or not? \n",
    "        * And then there will be other words, for example, there might be ones somewhere down here which is predicting, is the word king or not and so on, for these possible words in your vocabulary. \n",
    "    * but instead of training all 10,000 of them \n",
    "        * on every iteration, we're only going to train five of them. \n",
    "            * We're going to train the one responding to the actual target word we got \n",
    "            * and then train four randomly chosen negative examples.    \n",
    "\n",
    "**Thus**\n",
    "* instead of having one giant 10,000 way Softmax, which is very expensive to compute, \n",
    "* we've instead turned it into 10,000 binary classification problems, each of which is quite cheap to compute. \n",
    "    * And on every iteration, we're only going to train five of them or more generally, k + 1 of them, \n",
    "        * of k negative examples \n",
    "        * and one positive examples.\n",
    "    *  k + 1 binary classification problems\n",
    "    \n",
    "**How do you choose the negative examples?**\n",
    "* Option 1 (extreme): Sample t's according to emperical words frequencies in the corpus\n",
    "    * PRoblem: very high representation of words the, and, etc\n",
    "* Option 2 (extreme): Sample uniformily random: 1/|c=vocab size|\n",
    "* Proposal: heuristic value: sample proportionally to the observed (in the corpus) frequency (f(Wi)) of the word to the power of 3/4\n",
    "\n",
    "<img src='./Images/W2_18.png' style=\"width: 60%\"></img> \n",
    "\n",
    "\n",
    "### (video 8) GloVe World vectors\n",
    "\n",
    "Another algorithm that has some momentum in the NLP community for computing words embeddings.\n",
    "* Used less than\n",
    "    * Word2Vec \n",
    "    * or the skip-gram models\n",
    "* But is simple \n",
    "\n",
    "Algorithm:\n",
    "* c, t\n",
    "* X<sub>ij</sub> = the number of times that a word j (target) appears in the context of i (context)\n",
    "* depending on the definition of context and target words, you might have that X<sub>ij</sub> equals X<sub>ji</sub> or not\n",
    "* using the gradient descent we search to mimimize the function\n",
    "    * ie you just want to learn vectors, so that their end product is a good predictor for how often the two words occur together. \n",
    "\n",
    "**Details**\n",
    "\n",
    "<img src='./Images/W2_19.png' style=\"width: 60%\"></img> \n",
    "\n",
    "* if X_ij is equal to zero, then log of 0 is undefined, is negative infinity. \n",
    "    * And so, what we do is, we want sum over the terms where X_ij is equal to zero. \n",
    "    * And so, what we're going to do is, add an extra weighting term. \n",
    "    * So this is going to be a weighting term, and this will be equal to zero if X_ij is equal to zero. \n",
    "    * And we're going to use a convention that zero log zero is equal to zero. \n",
    "* So what this means is, that if X_ij is equal to zero, just don't bother to sum over that X_ij pair. \n",
    "    * So then this log of zero term is not relevant. \n",
    "    * So this means the sum is sum only over the pairs of words that have co-occurred at least once in that context-target relationship. \n",
    "* The other thing that F(X_ij) does \n",
    "    * the weighting factor can be a function that gives a meaningful amount of computation, even to the less frequent words like durion, \n",
    "    * and gives more weight but not an unduly large amount of weight to words like, this, is, of, a, which just appear lost in languag\n",
    "* There are various heuristics for choosing this weighting function F    \n",
    "    \n",
    "Another thing is:    \n",
    "* theta<sub>i</sub> and e<sub>j</sub> are symmetric in that, if you look at the math, they play pretty much the same role and you could reverse them or sort them around, and they actually end up with the same optimization objective\n",
    "    * One way to train the algorithm is to initialize theta and e both uniformly around gradient descent to minimize its objective, \n",
    "    * and then when you're done for every word, to then take the average.\n",
    "\n",
    "   \n",
    "**What happens is, you cannot guarantee that the individual components of the embeddings are interpretable?**\n",
    "\n",
    "<img src='./Images/W2_20.png' style=\"width: 60%\"></img> \n",
    "\n",
    "<img src='./Images/W2_21.png' style=\"width: 60%\"></img> \n",
    "\n",
    "* with an algorithm like this, you can't guarantee that the axis used to represent the features will be well-aligned with what might be easily humanly interpretable axis.\n",
    "* the first feature might be a combination of gender, and royal, and age, and food, and cost, and size, is it a noun or an action verb, and all the other features.\n",
    "* but despite this type of linear transformation, the parallelogram map that we worked out when we were describing analogies, that still works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0ab8a",
   "metadata": {},
   "source": [
    "### (video 9) Sentiment classification\n",
    "\n",
    "**Definition:** task of looking at a piece of text and telling if someone likes or dislikes the thing they're talking about\n",
    "\n",
    "**Usage:** if you can train a system to map from X or Y based on a label data set like this, then you could use it to monitor comments that people are saying about maybe a restaurant that you run.\n",
    "\n",
    "**Challenge** of sentiment classification is you might not have a huge label training set for it. ut with word embeddings, you're able to build good sentiment classifiers even with only modest-size label training sets.\n",
    "\n",
    "<img src='./Images/W2_22.png' style=\"width: 60%\"></img> \n",
    "\n",
    "**Simple model**\n",
    "* \"The desert is excellent\"\n",
    "* vocab word indicies (small vocab)\n",
    "* one hot vectors \n",
    "* multiplied by embedding matrix learge on a huge dataset (knowledge from infrequent words)\n",
    "* => embedding vectors\n",
    "* summ/average embedding vectors = > 300 dim feature vector\n",
    "* => passed to softmax classifier that would give y_hat (probabilities of star classes)\n",
    "* softmax classifier summs/averages the meanings of all the words in the example\n",
    "\n",
    "Problem with algorithm: ignors word's order: \"Completely lacking in good taste, good service, and good ambiance\". => many \"goods\" => averaging will give positive rating\n",
    "\n",
    "<img src='./Images/W2_23.png' style=\"width: 60%\"></img> \n",
    "\n",
    "**More sophisticated model**\n",
    "* RNN can be used instead of summarization\n",
    "    * For each word find a one-hot vector x У => embedding vects\n",
    "    * Feed in RNN => computes the representation of the last timestep\n",
    "    * => softmax => y_hat\n",
    "* takes word sequence into account\n",
    "\n",
    "<img src='./Images/W2_24.png' style=\"width: 60%\"></img> \n",
    "\n",
    "\n",
    "### (video 10) Debiasing word embeddings\n",
    "\n",
    "* We'd like to make sure that word embeddings would be as much as possible free of undesirable forms of bias (gender, ethnicity, etc) \n",
    "\n",
    "<img src='./Images/W2_25.png' style=\"width: 60%\"></img> \n",
    "\n",
    "**Approach:**\n",
    "* First: identify the direction corresponding to a particular bias we want to reduce or eliminate\n",
    "    * How? For the case of gender: \n",
    "        * take the embedding vector for he and subtract the embedding vector for she,\n",
    "        * male and female, etc. \n",
    "        * Because that differs by gender\n",
    "        * Average them: allows you to figure out what the gender direction looks like\n",
    "    * Bias direction (1 dim subspace) + non-bias direction (299 dim subspace)\n",
    "        * The bias direction can be higher than 1-dimensional, \n",
    "        * and rather than take an average, better to use a more complicated algorithm called a SVU, a singular value decomposition\n",
    "* Second: neutralization step\n",
    "    * for every word that's not definitional (grandmother, grandfather, girl, boy, she, he, a gender is intrinsic in the definition), project it to get rid of bias.\n",
    "* Third: Equalization\n",
    "    * you might have pairs of words such as grandmother and grandfather, or girl and boy, where you want the only difference in their embedding to be the gender.\n",
    "    * the distance, or the similarity, between babysitter and grandmother is smaller than the distance between babysitter and grandfather.\n",
    "    * this maybe reinforces a bias that grandmothers end up babysitting more than grandfathers\n",
    "    * we'd like to make sure that words like grandmother and grandfather are both exactly the same similarity, or exactly the same distance, from words that should be gender neutral, such as babysitter or such as doctor.\n",
    "    *  we move grandmother and grandfather to a pair of points that are equidistant from this axis in the middle\n",
    "\n",
    "<img src='./Images/W2_26.png' style=\"width: 60%\"></img> \n",
    "\n",
    "How do you decide what word to neutralize?\n",
    "* doctor vs grandmother vs beared\n",
    "* authors trained a classifier to try to figure out what words are definitional, what words should be gender-specific and what words should not be\n",
    "*  it turns out that most words in the English language are not definitional, meaning that gender is not part of the definition\n",
    "*  so a linear classifier can tell you what words to pass through the neutralization step to project out this bias direction on to this 299-dimensional subspace\n",
    "* the number of pairs you want to equalize is actually also relatively small (at least for the gender), it is quite feasible to hand-pick most of the pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd0dc7",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "<img src='./Images/Q2_1.png'></img> \n",
    "<img src='./Images/Q2_2.png'></img> \n",
    "<img src='./Images/Q2_3.png'></img> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e618a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
