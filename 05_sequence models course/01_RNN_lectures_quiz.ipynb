{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12dcdbbd",
   "metadata": {},
   "source": [
    "# Week 1: RNN\n",
    "\n",
    "### (video 1) Sequence problems\n",
    "Examples of sequence data\n",
    "- speach recognition: x(audio = sequence) => y(transcript = sequence)\n",
    "- music generation: x(nan; single integer, referring to a style) => y(sequence) (T<sub>x</sub> ≠ T<sub>y</sub>)\n",
    "- sentiment classification: x(prase = sequence) => y(stars) (T<sub>x</sub> ≠ T<sub>y</sub>)\n",
    "- DNA sequence analysis: x(DNA code = sequence) => y (which part corresponds to a protein = sequence)\n",
    "- machine translation: x(phrase = sequence) => y(translation = sequence) (T<sub>x</sub> ≠ T<sub>y</sub>)\n",
    "- video activity recognition: x(videoframes = sequence) => y(activity)\n",
    "- NER: x(phrase = sequence) => y(entities)\n",
    "\n",
    "Problems can be addressed as supervised learning with labled data X, and Y as a training set\n",
    "=> but different types of sequence problems\n",
    "- X and Y are sequences\n",
    "- X and Y can have different length or same \n",
    "- only X or Y can be the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba828228",
   "metadata": {},
   "source": [
    "### (video 2) Notations to define sequence problems\n",
    "NER: identify people\n",
    "\n",
    "\n",
    "|  | word | word | word | word | word | word | word | word | word |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| input x: | Harry | Potter | and | Hermione | Granger | invented | a | new | spell |\n",
    "| input features: | x<sup><1></sup> | x<sup><2></sup>  | ... | ... | x<sup>&lt;t&gt;</sup>  | ... | ... | x<sup><8></sup>  | x<sup><9></sup>  |\n",
    "| output y1: | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 |\n",
    "| output: | y<sup><1></sup> | y<sup><2></sup>  | ... | ... | y<sup>&lt;t&gt;</sup>  | ... | ... | y<sup><8></sup> | y<sup><9></sup>  |\n",
    "\n",
    "y1 – not the best representation, since doesn't tell you, where is the start and end of peoples' names\n",
    "\n",
    "the input is 9 words => we'll have 9 sets of features representing these\n",
    "- x<sup>&lt;t&gt;</sup> – feature in the middle of the sequence\n",
    "- *t* implies that these are temporal sequences (but it will be used regardless the type of sequence)\n",
    "- T<sub>x</sub> – length of the input sequence = 9\n",
    "- T<sub>y</sub> – length of the output sequence = 9\n",
    "- T<sub>x</sub> and T<sub>y</sub> can be different\n",
    "        \n",
    "- X<sup>(i)</sup> – i-th training example, in this case, this particular phrase\n",
    "- x<sup>(i)&lt;t&gt; </sup> – t-th element of an input sequence of i-th training example   \n",
    "- T<sub>x</sub><sup>(i)</sup> – the length of an input sequence of i-th training example  \n",
    "- y<sup>(i)&lt;t&gt; </sup> – t-th element of an output sequence of i-th training example \n",
    "- T<sub>y</sub><sup>(i)</sup> – the length of an output sequence of i-th training example\n",
    "    \n",
    "**Representation of individual words of the sentence**\n",
    "- You come up with a vocabulary = dictionary\n",
    "- Usually 30-50k words, around 100k – not uncommon as well\n",
    "    \n",
    "| words | number |\n",
    "| --- | --- |\n",
    "|a| 1 |\n",
    "|aaron| 2 |\n",
    "|...| ... |\n",
    "|and| 367 |\n",
    "|...| ... |\n",
    "|harry| 4075 |\n",
    "|...| ... |\n",
    "|potter| 6830 |\n",
    "|...| ... |\n",
    "|zulu| 10000 |\n",
    "\n",
    "- to build this dictionaly: one of ways: find top 10k occuring words\n",
    "- then use one hot representations to encode each of this words:\n",
    "    - x<sup><1></sup> = vetor [0, ... , 1, ...., 0] with unique 1 in 4075-th position\n",
    "\n",
    "And the *goal* is given this representation for X to learn a mapping using a sequence model to then target output y, I will do this as a supervised learning problem, I'm sure given the table data with both x and y.\n",
    "    \n",
    "If you encounter a word that is not in your vocabulary, you create a new token or a new fake word called *Unknown* = UNK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e8054",
   "metadata": {},
   "source": [
    "### (video 3) Recurrent Neural Network (RNN) model\n",
    "\n",
    "**Why not standard network?**\n",
    "\n",
    "9 input words = [x<sup><1></sup>, ..., x<sup>&lt;t&gt;</sup>, ..., x<sup>&lt;T<sub>x</sub>&gt;</sup>]\n",
    "\n",
    "feed them into standard NN of a few layers => \n",
    "output of 9 values 0 or 1 = [y<sup><1></sup>, ..., y<sup>&lt;t&gt;</sup>, ..., y<sup>&lt;T<sub>y</sub>&gt;</sup>] \n",
    "    \n",
    "which gonna tell you whether each of those words are part of persons name\n",
    "\n",
    "But this not gonna work, cause:\n",
    "- Inputs and outputs can be of different lengths in different examples (and even no maximum sentence length is known)\n",
    "- Doesn't share feautures learnt across different positions of text: \"Harry\" in first position recognized as a name won't imply that \"Harry\" in some other position will be recognized as such\n",
    "\n",
    "Similarly to CNN \n",
    "- you want things learned for one part of the image to generalize quickly to other parts of the input (in CNN - image, in SM - sequence)\n",
    "- better representation will also let you reduce the number of parameters in your model\n",
    "\n",
    "If we think of reqular network: each of x<sup>&lt;t&gt;</sup> is a 10k onehot encoded vector => so the total input size would be 10k x max amount of words in phrase => the weight matrix would be too huge\n",
    "\n",
    "**What is RNN?**\n",
    "- We read sentence from left to right\n",
    "- We take first word x<sup>&lt;1&gt;</sup> and feed it to a first NN layer. We try to predict an output y_hat<sup>&lt;1&gt;</sup>: whether it is a part of person's name or not\n",
    "- RNN when it goes to read second word x<sup>&lt;2&gt;</sup>, \n",
    "    - instead of just predicting y_hat<sup>&lt;2&gt;</sup> using x<sup>&lt;2&gt;</sup>, \n",
    "    - it also gets some information from the comptuation of step 1\n",
    "    - in particular: an **activation** value from time step one is passed to time step 2: a<sup>&lt;1&gt;</sup>\n",
    "- and so on for each next word: input x<sup>&lt;t&gt;</sup> => y_hat<sup>&lt;t&gt;</sup>\n",
    "- till the last ont: input x<sup>&lt;T<sub>x</sub>&gt;</sup> => y_hat<sup>&lt;T<sub>y</sub>&gt;</sup>\n",
    "- In this example, T<sub>x</sub> = T<sub>y</sub>, but the architecture will change a bit if T<sub>x</sub> ≠ T<sub>y</sub>\n",
    "- At the begining we pass an activation a<sup>&lt;0&gt;</sup> which can be \n",
    "    - randomly initiated\n",
    "    - vector of zeros\n",
    "    \n",
    "Representation of RNN: \n",
    "\n",
    "x<sup>&lt;t&gt;</sup> => [ooooooooo] (loop + shaded box = time delay of one step) => y<sup>&lt;t&gt;</sup>\n",
    "\n",
    "**Parameters**\n",
    "- The recurrent neural network scans through the data from left to right. \n",
    "- The parameters it uses for each time step are shared  \n",
    "    - w<sub>ax</sub> = govern the connection from X1 to the hidden layer\n",
    "        - The second index means that this w<sub>ax</sub> is going to be multiplied by some X-like quantity\n",
    "        - \"a\" means that this is used to compute some a-like quantity (see equations below)\n",
    "    - w<sub>aa</sub> =  govern the horizontal connections = the activations\n",
    "    - w<sub>ya</sub> = govern the output predictions\n",
    "\n",
    "So, to make a prediction for y_hat<sup>&lt;3&gt;</sup>, this RNN gets the information not only from x<sup>&lt;3&gt;</sup> but also the information from x<sup>&lt;1&gt;</sup> and x<sup>&lt;2&gt;</sup> because the information on x<sup>&lt;1&gt;</sup> can pass horizontally\n",
    "\n",
    "one **weakness** of this RNN is that it only uses the information that is earlier in the sequence to make a prediction. This can be overcome in Bidirectional RNN\n",
    "\n",
    "**What are the calculations this RNN does?**\n",
    "- a<sup>&lt;0&gt;</sup> = [0] (vector)\n",
    "- a<sup>&lt;1&gt;</sup> = g(w<sub>aa</sub> * a<sup>&lt;0&gt;</sup> + w<sub>ax</sub> * x<sup>&lt;1&gt;</sup> + b<sub>a</sub>)\n",
    "    - g = activation function, \n",
    "        - often is \"tanh\" \n",
    "        - RelU (preventing the vanishing gradient problem)\n",
    "    - b<sub>a</sub> = bias    \n",
    "- y_hat<sup>&lt;1&gt;</sup> = f(w<sub>ya</sub> * a<sup>&lt;1&gt;</sup> + b<sub>y</sub>)\n",
    "    - f = activation function (same or another), depends on what the output is\n",
    "        - binary classification problem (NER) = sigmoid activation function\n",
    "        - k-way classification problem = softmax\n",
    "    - b<sub>y</sub> = bias\n",
    "    \n",
    "More generaly \n",
    "- a<sup>&lt;t&gt;</sup> = g(w<sub>aa</sub> * a<sup>&lt;t-1&gt;</sup> + w<sub>ax</sub> * x<sup>&lt;t&gt;</sup> + b<sub>a</sub>)\n",
    "- y_hat<sup>&lt;t&gt;</sup> = f(w<sub>ya</sub> * a<sup>&lt;t&gt;</sup> + b<sub>y</sub>)\n",
    "\n",
    "Simplification of these two equations\n",
    "- a<sup>&lt;t&gt;</sup> = g (w<sub>a</sub> * [a<sup>&lt;t-1&gt;</sup>, x<sup>&lt;t&gt;</sup>]+ b<sub>a</sub>)\n",
    "    - w<sub>a</sub> is a matrix which is defined as a horizontal stack of matricies [w<sub>aa</sub>; w<sub>ax</sub>]\n",
    "    - if dim(a) = 100 and dim(x) =10k => \n",
    "        - dim(w<sub>aa</sub>) = 100; 100\n",
    "        - dim(w<sub>ax</sub>) = 100; 10k\n",
    "        - dim(w<sub>a</sub>) = 100; 10100\n",
    "    - [a<sup>&lt;t-1&gt;</sup>, x<sup>&lt;t&gt;</sup>] is two vectors stacked together vertically \n",
    "        - dim(a<sup>&lt;t-1&gt;</sup>) = 100, 1\n",
    "        - dim(x<sup>&lt;t&gt;</sup>) = 10k, 1\n",
    "        - dim([a<sup>&lt;t-1&gt;</sup>, x<sup>&lt;t&gt;</sup>]) = 10100, 1\n",
    "    - The advantage of this notation is that rather than carrying around two parameter matrices, Waa and Wax, we can compress them into just one parameter matrix Wa\n",
    "- y_hat<sup>&lt;t&gt;</sup> = f(w<sub>y</sub> * a<sup>&lt;t&gt;</sup> + b<sub>y</sub>) (similarly)\n",
    "    - w<sub>y</sub> and b<sub>y</sub> denotes what type of output quantity we're computing.\n",
    "        - w<sub>y</sub> indicates a weight matrix or computing a y-like quantity, \n",
    "        - w<sub>a</sub> and b<sub>a</sub>  on top indicates that thes eparameters are for computing activation output quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210ee42",
   "metadata": {},
   "source": [
    "### (video 4) Back propagation through time\n",
    "\n",
    "**Element-wise (individual time steps) loss function**\n",
    "\n",
    "A certain word in the sequence is supposed to be a person's name: y<sup>&lt;t&gt;</sup> = 1\n",
    "And the NN outputs some probability of the particular word being a person's name: y_hat<sup>&lt;t&gt;</sup> = 0.1\n",
    "Loss = standard logistic regression loss, also called the cross entropy loss\n",
    "\n",
    "L<sup>&lt;t&gt;</sup>(y_hat<sup>&lt;t&gt;</sup>, y<sup>&lt;t&gt;</sup>) = - y<sup>&lt;t&gt;</sup> * log(y_hat<sup>&lt;t&gt;</sup>) - (1-y<sup>&lt;t&gt;</sup>) * log(1-y_hat<sup>&lt;t&gt;</sup>)\n",
    "\n",
    "**Overall loss**\n",
    "\n",
    "L(y_hat, y) = sum(t=1>T<sub>y</sub>) [L<sup>&lt;t&gt;</sup>(y_hat<sup>&lt;t&gt;</sup>, y<sup>&lt;t&gt;</sup>)]\n",
    "\n",
    "Backprop requires doing computations in the opposite directions. And that then, allows you to compute all the appropriate quantities that lets you then, take derivatives, respected parameters, and update the parameters using gradient descent\n",
    "\n",
    "In back propagation procedure, the most significant message (or the most significant recursive calculation) is the one between activations. And it gives this algorithm the name \"backpropagation through time\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d561aa0",
   "metadata": {},
   "source": [
    "### (video 5) Different Types of RNNs\n",
    "\n",
    "**Example 1 (previous): many-to-many architecture with T<sub>x</sub> = T<sub>y</sub>**\n",
    "\n",
    "<img src='./Images/0004.png'></img>\n",
    "\n",
    "**Example 2: sentiment classification: many-to-one **\n",
    "\n",
    "* x = text = \"There is nothing to like in this movie.\n",
    "* y = 0/1 (positive or negative review) or [1, 5]\n",
    "\n",
    "Simplification of the architecture: rather than having to use an output at every single time-step, we can then just have the RNN read into entire sentence and have it output y at the last time-step when it has already input the entire sentence\n",
    "\n",
    "<img src='./Images/0003.png'></img>\n",
    "\n",
    "**Example 3: standard NN covered in first two courses: one-to-one**\n",
    "\n",
    "<img src='./Images/0001.png'></img>\n",
    "\n",
    "**Example 4: music generation: one-to-many**\n",
    "\n",
    "* input = x = null or any integer defining genre or smth else\n",
    "* output = set of notes of the musical piece. \n",
    "* If you want, you can have this input a(0) as well\n",
    "* technically when you're actually generating sequences, often you take these first synthesized output and feed it to the next layer as well, and so on\n",
    "\n",
    "<img src='./Images/0002.png'></img>\n",
    "\n",
    "**Example 5: machine translation: many-to-many architecture with T<sub>x</sub> ≠ T<sub>y</sub>**\n",
    "\n",
    "2 distinct parts of the RNN: \n",
    "* the encoder \n",
    "* the decoder\n",
    "\n",
    "<img src='./Images/0005.png'></img>\n",
    "\n",
    "**Example 6: will be shown on week 4**\n",
    "\n",
    "Technically, there's one other architecture – attention based architectures. It isn't cleanly captured by one of the diagrams we've drawn so far.\n",
    "\n",
    "**NB** there are some subtleties with sequence generation, which is what we'll discuss in the next video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc7231a",
   "metadata": {},
   "source": [
    "### (video 6) Language Model and Sequence Generation\n",
    "\n",
    "Language modeling is one of the most basic and important task in natural language processing. It is also one that RNNs do very well. In this video, you'll learn about how to build a language model using an RNN\n",
    "\n",
    "**What is a language model?**\n",
    "\n",
    "* the apple and pear salad was delicious. \n",
    "    * What did you just hear me say? \n",
    "        * Did I say the apple and pair salad? \n",
    "        * Or did I say the apple and pear salad?\n",
    "* Speech recognition system picks the second sentence by using language model which tells it what is the probability of either of these two sentences.\n",
    "* language model, given any sentence, tells you what is the probability of that particular sentence\n",
    "    * probability of sentence = if you were to pick up a random newspaper, open a random email, or pick a random webpage, or listen to the next thing someone says, what is the chance that the next sentence you read somewhere out there in the world will be a particular sentence?\n",
    "* fundamental component for both \n",
    "    * speech recognition systems \n",
    "    * for machine translation systems\n",
    "    \n",
    "<img src='./Images/0006.png'></img>\n",
    "    \n",
    "P(sentence) = P(y<sup>&lt;1&gt;</sup>, y<sup>&lt;2&gt;</sup>, ..., y<sup>&lt;T<sub>y</sub>&gt;</sup>)\n",
    "\n",
    "= probability of that particular sequence of words\n",
    "\n",
    "**How do you build a language model?**\n",
    "\n",
    "* (1) Training set: large corpus of text in a certain language\n",
    "* Let's say you get a sentence in your training set as follows: \"cats average 15 hours of sleep a day\"\"\n",
    "* (2) You tokenize it: form a vocab and encode each word with one hot vectors\n",
    "    * You model when the sentence ends: form an exra token: < EOS > token\n",
    "        * It's not necessary \n",
    "        * Period can be a token as well\n",
    "    * If there is a word in the sentence abscent in the vocab = < UNK > token\n",
    "        * and we just model the chance of the unknown word instead of the specific word\n",
    "* (3) build an RNN to model the chance of these different sequences\n",
    "    * (input) x<sup>&lt;t&gt;</sup> = y<sup>&lt;t-1&gt;</sup> (output)\n",
    "    * At time zero, you're going to end up computing some activation a<sup>&lt;1&gt;</sup> as a function of some input x<sup>&lt;1&gt;</sup>\n",
    "    * and x<sup>&lt;1&gt;</sup> would just be set to zero vector. \n",
    "    * The previous a<sup>&lt;1&gt;</sup> by convention, also set that to vector zeros. \n",
    "    * But what a<sup>&lt;1&gt;</sup> does is it will make a Softmax prediction to try to figure out what is the probability of the first word y, so that's going to be y_hat<sup>&lt;1&gt;</sup>. \n",
    "        * What this step does is really it has a Softmax, \n",
    "        * so it's trying to predict what is the probability of any word in a dictionary, \n",
    "        * what's the chance that the first word is a, what's the chance that the first word is Aaron, \n",
    "        * what's the chance that the first word is cats, \n",
    "        * what's the chance the first word is Zulu,\n",
    "        * what's the chance that the first word is an unknown word, \n",
    "    * y_hat<sup>&lt;1&gt;</sup> is output according to a Softmax, it just predicts what's the chance that the first word being \"cats\"\n",
    "    * At the second step, the output is again predicted by Softmax, the RNN's job is to predict what's the chance of it being whatever word it is, given what had come previously: cats\n",
    "    \n",
    "<img src='./Images/0007.png'></img>\n",
    "\n",
    "* (4) Define the cost function\n",
    "    *  At a certain time t, if the true word was y<sup>&lt;t&gt;</sup> and your network Softmax predicted some y_hat<sup>&lt;t&gt;</sup>, then this is the Softmax loss function \n",
    "    * and then the overall loss is just the sum over all time steps of the losses associated with the individual predictions.\n",
    "    * Given the sentence \"y<sup>&lt;1&gt;</sup> y<sup>&lt;2&gt;</sup> y<sup>&lt;3&gt;</sup>)\"\n",
    "    * The first softmax tells the probability of the word y<sup>&lt;1&gt;</sup>\n",
    "    * And we end up with P(y<sup>&lt;1&gt;</sup>, y<sup>&lt;2&gt;</sup>, y<sup>&lt;3&gt;</sup>) = P(y<sup>&lt;1&gt;</sup>) * P (y<sup>&lt;2&gt;</sup>| y<sup>&lt;1&gt;</sup>) * P (y<sup>&lt;3&gt;</sup>| y<sup>&lt;1&gt;</sup>, y<sup>&lt;2&gt;</sup>)\n",
    "    \n",
    "<img src='./Images/0008.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba01402",
   "metadata": {},
   "source": [
    "### (video 7) Sampling Novel Sequences\n",
    "\n",
    "After you train a sequence model, one of the ways you can informally get a sense of what is learned is to have a sample novel sequences\n",
    "\n",
    "Sequence model, models the chance of any particular sequence of words. We like it to do is sample from this distribution to generate noble sequences of words.\n",
    "\n",
    "<img src='./Images/0009.png'></img>\n",
    "\n",
    "The network was trained using this structure shown at the top. But to sample, you do something slightly different\n",
    "* you input the usual \n",
    "    * x<sup>&lt;1&gt;</sup> = 0, \n",
    "    * a<sup>&lt;1&gt;</sup> = 0. \n",
    "* And now your first time stamp will have softmaxmax probability over possible outputs (vector of word probabilities)\n",
    "    * And then you take this vector and use, for example, the numpy command np.random.choice to sample the first word according to distribution defined by this vector probabilities.\n",
    "* The second time step is expecting this y_hat<sup>&lt;1&gt;</sup> as input.\n",
    "    * The soft max will make a prediction for what is y_hat<sup>&lt;2&gt;</sup> \n",
    "    * The neuron will give you the vector of probabilities for all vocab words given the chosen first word \n",
    "    * And you sample from most probable words of this vector\n",
    "* If the end of sentence token is part of your vocabulary, you could keep sampling until you generate an EOS token\n",
    "    *  alternatively, you can just decide to sample 20 words and keep going until you've reached that number of time steps\n",
    "*  this particular procedure will sometimes generate an unknown word token. If you want to make sure that your algorithm never generates tit, you can reject any sample that came out as unknown word token and just keep resampling from the rest of the vocabulary until you get a word that's not an unknown word.\n",
    "\n",
    "\n",
    "Depending on your application, one thing you can do is also build a **character level RNN**\n",
    "\n",
    "Vocab = [a, ... , z, , . , ; , 0, ... ,9, A, ....]\n",
    "\n",
    "Your sequence y<sup>&lt;1&gt;</sup>, y<sup>&lt;2&gt;</sup>, y<sup>&lt;3&gt;</sup> would be the individual characters in your training data, rather than the individual words in your training data\n",
    "\n",
    "Using a character level language model has some pros and cons. \n",
    "* Pros\n",
    "    * you don't ever have to worry about unknown word tokens. \n",
    "        * the model is able to assign a sequence like mau, a non-zero probability. \n",
    "        * Whereas if mau was not in your vocabulary for the word level language model, you just have to assign it the unknown word token. \n",
    "* Cons\n",
    "    * you end up with much longer sequences: many english sentences will have 10 to 20 words but may have many, many dozens of characters. \n",
    "    * Athe model is not as good as word level language models at capturing long range dependencies between how the the earlier parts of the sentence also affect the later part of the sentence. \n",
    "    * And character level models are also just more computationally expensive to train. \n",
    "\n",
    "So the trend in NLP is that for the most part, word level language model are still used, but as computers gets faster there are more and more applications where people are, at least in some special cases, starting to look at more character level models.\n",
    "\n",
    "**Further we'll discuss challenges of training RNNs and how to deal with them**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2b8fd",
   "metadata": {},
   "source": [
    "### (video 8) Vanishing Gradients problem with RNNs\n",
    "\n",
    "* The **cat**, which already ate ...., **was** full. \n",
    "* The **cats**, which already ate ..., **were** full.\n",
    "\n",
    "in English this stuff in the middle (...) could be arbitrarily long.\n",
    "\n",
    "The basic RNN we've seen so far is not very good at capturing very long-term dependencies\n",
    "* The same problem of vanishing gradients appears for very deep neural networks. \n",
    "    * You would carry out forward prop from left to right and then backprop. \n",
    "    * If this is a very deep neural network, then the gradient from this output y would have a very hard time propagating back to affect the weights of these earlier layers, to affect the computations of the earlier layers.\n",
    "* RNN has similar problem \n",
    "    * It can be quite difficult for the outputs of the errors associated with the later timesteps to affect the computations that are earlier. \n",
    "    * In practice, what this means is it might be difficult to get a neural network to realize that it needs to memorize.\n",
    "    \n",
    "In basic RNN model has many local influences: the output y_hat<sup>&lt;3&gt;</sup> is mainly influenced by values close to y_hat<sup>&lt;3&gt;</sup>\n",
    "    \n",
    "**For very Deep NN we also talked about exploding gradients**\n",
    "\n",
    "* While doing backprop, the ingredients should not just decrease exponentially they may also increase exponentially with the number of layers you go through. \n",
    "    * It turns out that vanishing gradients tends to be the biggest problem with training RNNs. \n",
    "    * Although when exploding gradients happens it can be catastrophic because the exponentially large gradients can cause your parameters to become so large that your neural network parameters get really messed up.\n",
    "    \n",
    "* It turns out that exploding gradients are easier to spot because the parameter has just blow up. \n",
    "    * You might often see NaNs, not a numbers, meaning results of a numerical overflow in your neural network computation. \n",
    "    * If you do see exploding gradients, one solution to that is apply **gradients clipping**\n",
    "        * look at your gradient vectors, \n",
    "        * and if it is bigger than some threshold, \n",
    "        * re-scale some of your gradient vectors so that it's not too big, \n",
    "        * so that is clipped according to some maximum value.\n",
    "* vanishing gradients is much harder to solve, which we are going to discuss in the next sections "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e15bd8",
   "metadata": {},
   "source": [
    "### (video 9) Gated Recurrent Unit (GRU)\n",
    "\n",
    "* GRU is a modification to the RNN hidden layer that makes it much better at capturing long-range connections and helps a lot with the vanishing gradient problem\n",
    "\n",
    "You've already seen the formula for computing the activations at time t of an RNN. \n",
    "* It's the activation function applied to \n",
    "    * the parameter W_a \n",
    "        * times the activations for a previous time sediment the current input \n",
    "    * plus the bias. \n",
    "\n",
    "<img src='./Images/0010.png'></img>\n",
    "\n",
    "we're going to use a similar picture to explain the GRU\n",
    "\n",
    "* The cat, which already ate ...., was full.\n",
    "* The cats, which already ate ..., were full.\n",
    "\n",
    "\n",
    "**Simplified GRU**\n",
    "\n",
    "the GRU unit is going to have a **new variable called C**, which stands for cell, for memory cell. \n",
    "* What the memory cell do is it will provide a bit of memory. \n",
    "* Remember, for example, whether cat was singular or plural, so that when it gets much further into the sentence, it can still work on the consideration whether the subject of the sentence was singular or plural.\n",
    "* At time t, the memory cell will have some value **с<sup>&lt;t&gt;</sup>**\n",
    "* the GRU unit will actually output an activation value  **a<sup>&lt;t&gt;</sup>** =  **с<sup>&lt;t&gt;</sup>**\n",
    "    * different symbols c and a are used to denote the memory cell value and the output activation value even though they're the same \n",
    "    * I'm using this notation because when we talked about LSTMs a little bit later, these will be two different values.\n",
    "    \n",
    "These are the equations that govern the computations of a GRU unit. \n",
    "* At every time step, we're going to consider an overwriting the memory cell with a value c tilde of t. \n",
    "* This going to be a **candidate** for replacing c of t. \n",
    "* We're going to compute this using an activation function, tanh \n",
    "    * of w_c, and so that's the parameter matrix w_c and we'll pass it as parameter matrix.\n",
    "    \n",
    "<img src='./Images/0011.png'></img>    \n",
    "\n",
    "The important idea of the GRU, it will be that we'll have a **gate**. \n",
    "* The gate I'm going to call Gamma_u = a value belonging to [0; 1] = sigmoid function\n",
    "    * u stands for update gate\n",
    "    * Gamma is very close to 0 or to 1 most of the time\n",
    "* Then next the key part of the GRU is the equation\n",
    "    * we have come up with a candidate where we're thinking of updating C using c tilde \n",
    "    * and then the gate will decide whether or not we actually update it\n",
    "        * this memory cell C is going to be set to either zero or one depending on whether the word you're conserving, really the subject of the sentence is singular or plural\n",
    "        *  the GRU unit will memorize the value of the C^t all the way down, where this is still equal to one and so that tells it was singular so use the choice was. \n",
    "        * The job of the gate, of gamma u, is to decide when do you update this value.\n",
    "        * when you see the phrase the cat, you know that you're talking about a new concept, the subject of the sentence cat. That would be a good time to update this bit and then maybe when you're done using it the cat was full, then you know I don't need to memorize anymore I can just forget that.\n",
    "\n",
    "* The specific equation we'll use for the GRU is the following:\n",
    "\n",
    "<img src='./Images/0012.png'></img>   \n",
    "\n",
    "* if the gate is equal to one, then c<sup>&lt;t&gt;</sup> equal to this candidate value so that's like over here, set the gate equal to one so go ahead and update that bit\n",
    "* Then for all of these values in the middle, you should have the gate equal zero so do the same, don't update it\n",
    "\n",
    "<img src='./Images/0013.png'></img>   \n",
    "\n",
    "Simplified version of GRU unit (violet box is the equation for c<sup>&lt;t&gt;</sup> from the picture above):\n",
    "* the value of the memory cell from the previous step is taken along with an input\n",
    "* tanh is applied to calculate the candidate for the next value of a memory cell\n",
    "* sigmoid is applied to calculate the gate value, defining whether the candidate would be accepted\n",
    "* all of them are used to calculate \n",
    "    * the next value of memory cell \n",
    "    * as well as an output (after the application of softmax function)\n",
    "\n",
    "<img src='./Images/0014.png'></img>   \n",
    "\n",
    "* because the gate is quite easy to set to zero so long as sigmoid argument is a large negative value, \n",
    "    * then up to numerical round-off, the update gate will be essentially zero, very close to zero. \n",
    "    * When that's the case, then this update equation and sub setting c<sup>&lt;t&gt;</sup> = c<sup>&lt;t-1&gt;</sup> \n",
    "    * so this is very good at maintaining the value for the cell \n",
    "    * (!) and because gamma can be so close to zero, can be 0.000001 or even smaller than that, **it doesn't suffer from much of a vanishing gradient problem**\n",
    "    \n",
    "**Implementation**\n",
    "* In the equations have written, c<sup>&lt;t&gt;</sup> can be a vector (100 dim)\n",
    "* c_tilda<sup>&lt;t&gt;</sup> also 100 dim\n",
    "* Gamma also the same dimention of 100\n",
    "* So the multiplications in the  c<sup>&lt;t&gt;</sup> equation are element-wise multiplications\n",
    "* If Gamma = 100-dimensional vector of bits, the value is mostly 0 and 1, that tells you of this **100-dimensional memory cell**, which are the bits you want to update.\n",
    "    * these element-wise multiplications do is it just tells you GRU **which are the dimensions of your memory cell vector to update** at every time step. \n",
    "    * You can choose to keep some bits constant while updating other bits. \n",
    "    * For example, \n",
    "        * maybe you'll use one-bit to remember the singular or plural cat, \n",
    "        * and maybe you'll use some other bits to realize that you're talking about food. \n",
    "        * Because we talked about eating and talk about foods, then you'd expect to talk about whether the cat is full later\n",
    "        \n",
    "**Full GRU**\n",
    "* We add one more gate = Gamma_r\n",
    "    * r stands for relevance\n",
    "    * This gate Gamma r tells you how relevant is c<sup>&lt;t-1&gt;</sup> to computing the next candidate for c<sup>&lt;t&gt;</sup>\n",
    "\n",
    "<img src='./Images/0015.png'></img>    \n",
    "\n",
    "Why not use a simpler version from the previous section? Experiments show that it works better. Other common version is called an LSTM, which stands for Long, Short-term Memory\n",
    "\n",
    "*Notations: sometimes h_tilda, u, r, h are used in academic literature*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d5149",
   "metadata": {},
   "source": [
    "### (video 10) Long Short Term Memory (LSTM)\n",
    "\n",
    "More powerful and general than GRU\n",
    "\n",
    "<img src='./Images/0016.png'></img>\n",
    "\n",
    "* For the LSTM, we will no longer have the case that a&lt;t&gt; is equal to c&lt;t&gt;. \n",
    "* So this is what we use and so this is like the equation on the left except that \n",
    "    * we now more explicitly use a<sup>&lt;t-1&gt;</sup> not c<sup>&lt;t-1&gt;</sup>; \n",
    "    * and we are not using this gamma r this relevance\n",
    "* Update gate the same as before\n",
    "* But instead of having one update gate control both of these terms, we're going to have two separate terms. Forget gate Gamma_f. \n",
    "    * this gives the memory cell the option of keeping the old value c&lt;t-1&gt; and then just adding to it this new value c_tilda<sup>&lt;t&gt;</sup>.\n",
    "     * So use a separate update and forget gates right? \n",
    "* And new output gate\n",
    "\n",
    "**Variations**\n",
    "\n",
    "* **peephole connection** – the most common one\n",
    "    * instead of just having the gate values be dependent only on a<sup>&lt;t-1&gt;</sup>, x<sup>&lt;t&gt;</sup>, \n",
    "    * Sometimes people also sneak in there the value c<sup>&lt;t-1&gt;</sup>  as well\n",
    "    * these are vectors, so each X-th element of c<sup>&lt;t-1&gt;</sup> affects corresponding X-th elements of each gate \n",
    "\n",
    "<img src='./Images/0017.png'></img>\n",
    "\n",
    "**when should you use a GRU and when should you use an LSTM?** \n",
    "* There is a widespread consensus in this. \n",
    "* LSTMs actually came much earlier and then GRUs were relatively recent invention that were maybe derived as partly a simplification of the more complicated LSTM model. \n",
    "* Researchers have tried both of these models on many different problems and on different problems the different algorithms will win out. \n",
    "    * the advantage of the GRU is that it's a simpler model. And so it's actually easier to build a much bigger network only has two gates, so computation runs a bit faster so it scales the building, somewhat bigger models. \n",
    "    * But the LSTM is more powerful and more flexible since there's three gates instead of two. \n",
    "* If you want to pick one to use, I think LSTM has been the historically more proven choice. Although I think the last few years GRUs have been gaining a lot of momentum: it might be easier to scale them to even bigger problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d6f35",
   "metadata": {},
   "source": [
    "### (video 11) Bidirectional RNN\n",
    "\n",
    "<img src='./Images/0018.png'></img>\n",
    "\n",
    "these cells are standard RNN blocks, or GRU units, or LSTM blocks\n",
    "\n",
    "* So we have four inputs, X1 through X4. \n",
    "* So this network's hidden there will have a forward recurring components. So I'm going to call this a1, a2, a3, and a4. \n",
    "* And so each of these four recurrent units influence the current X.\n",
    "* To help predict y hat 1, y hat 2, y hat 3, and y hat 4\n",
    "* Plus backward recurrent component\n",
    "\n",
    "<img src='./Images/0019.png'></img>\n",
    "\n",
    "* This network defines an acyclic graph\n",
    "    * And so, given an input sequence X1 to X4, the four sequence we first compute a for (1), then use that to compute a for (2), then a for (3), then a for (4). \n",
    "    * Whereas the backward sequence will start by computing a backward four and then go back and compute a backward three. \n",
    "    * And notice your computing network activations. This is **not back prop!**, this is forward prop\n",
    "    * When you compute all forward and backward activations, you finally can get your predictions: taking into account past and future\n",
    "    \n",
    "if you have an NLP problem, and you have a complete sentence, you're trying to label things in the sentence, a bidirectional RNN with LSTM blocks would be a pretty reasonable first thing to try\n",
    "    \n",
    "**Disadvantage of BRNN**\n",
    "\n",
    "You do need the entire sequence of data before you can make predictions anywhere (speech recognition system won't work on straightforward application of BRNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b08e5",
   "metadata": {},
   "source": [
    "### (video 12) Deep RNNs\n",
    "\n",
    "The different versions of RNNs you've seen so far will already work quite well by themselves. But for learning very complex functions sometimes is useful to stack multiple layers of RNNs together to build even deeper versions of these models\n",
    "\n",
    "**standard neural network** \n",
    "* you will have an input X.\n",
    "* And then that's stacked to some hidden layer that might have activations a1 for the first hidden layer, \n",
    "* And then that's stacked to the next layer with activations a2, \n",
    "* then maybe another layer, activations a3 \n",
    "* and then you make a prediction ŷ. \n",
    "\n",
    "<img src='./Images/0020.png'></img>\n",
    "\n",
    "\n",
    "**deep RNNs**\n",
    "* a<sup>[ l ] &lt;t&gt;</sup>\n",
    "    * l = layer\n",
    "    * t = time\n",
    "* How a particular activation is computed: the same parameters Wa[l] and ba[l] are used for every one of these computations at the layer \"l\". \n",
    "\n",
    "<img src='./Images/0021.png'></img>\n",
    "\n",
    "* whereas for standard NNs we've seen neural networks that are very, very deep, maybe over 100 layers. \n",
    "* For RNNs, having three layers is already quite a lot. \n",
    "    * Because of the temporal dimension, these networks can already get quite big even if you have just a small handful of layers. \n",
    "    * And you don't usually see these stacked up to be like 100 layers.\n",
    "    * One thing you do see sometimes is that you have recurrent layers that are stacked on top of each other \n",
    "        * Plus then you might take an output and then just have a bunch of deep layers (on top of it) that are not connected horizontally but have a deep network, that then finally predicts y<1>.\n",
    "* blocks can be standard RNN blocks, or GRU units, or LSTM blocks\n",
    "\n",
    "**NB** deep RNNs are quite computationally expensive to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d456037",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "<img src='./Images/Q1_1.png'></img>\n",
    "<img src='./Images/Q1_2.png'></img>\n",
    "<img src='./Images/Q1_3.png'></img>\n",
    "<img src='./Images/Q1_4.png'></img>\n",
    "<img src='./Images/Q1_5.png'></img>\n",
    "<img src='./Images/Q1_6.png'></img>\n",
    "<img src='./Images/Q1_7.png'></img>\n",
    "<img src='./Images/Q1_8.png'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f954b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
