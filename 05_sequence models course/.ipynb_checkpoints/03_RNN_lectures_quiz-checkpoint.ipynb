{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47869d1b",
   "metadata": {},
   "source": [
    "# Week 3: Sequence to sequenсе architectures\n",
    "\n",
    "### (video 1) Basic models\n",
    "\n",
    "Translation:\n",
    "* Encoder network (RNN based on GRU or LSTM units)\n",
    "    * Input: word seq \n",
    "    * Output: vector representation\n",
    "* Decoder network\n",
    "    * Input: vector representation\n",
    "    * Output: translated seq\n",
    "\n",
    "<img src='./Images/W3_01.png' style=\"width: 60%\"></img>\n",
    "\n",
    "Image captioning: the same logic\n",
    "*  Encoder: eg pre-trained AlexNet, without final softmax unit\n",
    "    * Input: an image into a convolutional network  \n",
    "    * output: 4,096-dimensional feature vector of which to represent this picture of a cat.\n",
    "* Decoder network\n",
    "    * Input: vector representation\n",
    "    * Output: gernerated caption one word at a time\n",
    "    \n",
    "<img src='./Images/W3_02.png' style=\"width: 60%\"></img>\n",
    "\n",
    "But compared to decoder models (= seq generation = language models) you may be want \n",
    "* the most likely translation, not some translation\n",
    "* the best caption, not randomly choosen caption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9704ee",
   "metadata": {},
   "source": [
    "### (video 2) Picking the Most Likely Sentence\n",
    "\n",
    "Regular language models vs seq to seq\n",
    "* Language model allows you to estimate the probability of a sentence. You can also use this to generate nove sentences\n",
    "* Machine translation as building a conditional language model.\n",
    "    * decoded network looks pretty much identical to the language model\n",
    "    * instead of always starting along with the vector of all zeros, it has an encoded network that figures out some representation for the input sentence which it takes as an input\n",
    "    * instead of modeling the probability of any sentence, it is now modeling the probability oа output English translation, conditions on some input French sentence. \n",
    "\n",
    "<img src='./Images/W3_03.png' style=\"width: 60%\"></img>\n",
    "\n",
    "* what you do not want is to sample outputs at random\n",
    "* ou would like is to find the English sentence, y, that maximizes that conditional probability. \n",
    "* The most common algorithm for doing this is called **beam** search (see next video)\n",
    "\n",
    "<img src='./Images/W3_04.png' style=\"width: 60%\"></img>\n",
    "\n",
    "But why not to use greedy search? \n",
    "*  algorithm from computer science which says \n",
    "    * to generate the first word just pick whatever is the most likely first word according to your conditional language mode\n",
    "    * and then it picks the second most likely word knowing first + language model of the target language, etc\n",
    "* what you would really like is to pick the entire sequence of words, that maximizes the joint probability of the **whole** sentence\n",
    "    * the word \"going\" is most probable taking into account its probability in english lang model + the previous word. But the translation using it is less accurate\n",
    "\n",
    "<img src='./Images/W3_05.png' style=\"width: 60%\"></img>\n",
    "\n",
    "* Plus, the total number of combinations of words in the English sentence is exponentially large\n",
    "* So, if you have just 10,000 words in a dictionary and if you're contemplating translations that are up to ten words long, \n",
    "* => then there are 10000ˆ10 possible sentences that are ten words long\n",
    "* it is impossible to rate them all => which is why the most common thing to do is use an **approximate** search algorithm\n",
    "    * it will try, it won't always succeed, to pick the sentence, y, that maximizes that conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf01d0",
   "metadata": {},
   "source": [
    "### (video 3) Beam Search\n",
    "\n",
    "**First step**\n",
    "* The first thing Beam search has to do is try to pick the first words of the English translation, that's going to operate\n",
    "    * List of 10,000 words into vocabulary (ignore capitalization)\n",
    "    * First step: What's the probability of the first output y, given the input sentence x (the French sentence vector)\n",
    "    * whereas greedy search will pick only the one most likely words and move on, Beam Search instead can consider multiple alternatives\n",
    "    * The algorithm has parameter B = beam width (number of alternatives)\n",
    " \n",
    "<img src='./Images/W3_06.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**Second step**\n",
    "* For each of these three choices consider what should be the second word\n",
    "* So, to evaluate the probability of second word, it will use \n",
    "    * the french sentence input\n",
    "    * the first word\n",
    "* => we try to maximize the probability of two words, not just the second\n",
    "* We consider 3 * 10 0000 possibilities\n",
    "* And we pick the most probablle 3 combinations and take on to the next step of Beam search (some of the first words can thus be rejected)\n",
    "\n",
    "<img src='./Images/W3_07.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**NB:** because of beam width is equal to three, every step you instantiate three copies of the network to evaluate these partial sentence fragments and the output. \n",
    "\n",
    "**Third step**\n",
    "\n",
    "<img src='./Images/W3_08.png' style=\"width: 60%\"></img>\n",
    "\n",
    "The outcome of this process hopefully will be that adding one word at a time that Beam search will decide that Jane visits Africa in September will be terminated by the end of sentence symbol \n",
    "\n",
    "**NB:** If B=1 => the algorithm becoms greedy search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a7978",
   "metadata": {},
   "source": [
    "### (video 4) Refinments to Beam Search\n",
    "\n",
    "Some little changes, they'll make Beam search work even better\n",
    "\n",
    "**Length normalization**\n",
    "* Beam search maxiizes the probability of the output sequence given the input sentence\n",
    "* Which can be transformed into the product of probabilities of each output word given all previous output words and the input vector\n",
    "    * All these probabilities are numbers much less than 1\n",
    "    * It can be too small for floating representation to be storeв by PC accurately\n",
    "    * So we take the log of a product which becomes a sum of the logs of each members\n",
    "* Since log is monotonically increasing function, we know that maximizing logP(y|x) should give you the same result as maximizing P(y|x)\n",
    "\n",
    "There's one other change to `argmax sum function` that makes the machine translation algorithm work even better. \n",
    "* if you have a very long sentence, the probability of that sentence is going to be low because you're multiplying as many terms \n",
    "*  Thus this function has an undesirable effect that it may be unnaturally tend to prefer very short translations \n",
    "* the same thing is true the log of a probability. \n",
    "    * Values of probability are always less than or equal to one\n",
    "    * => logs are always negative\n",
    "    * so the more terms you add together, the more negative the function becomes\n",
    "* So you can normalize this by the number of words in translation => significantly reduces the penalty for putting longer translations\n",
    "    * Sometimes you use a softer approach to тщкmalization, by taking the T<sub>y</sub>ˆ(alpha), where alpha = 0.7\n",
    "    * There isn't a great theoretical justification for it, but people found this works well\n",
    "\n",
    "<img src='./Images/W3_09.png' style=\"width: 60%\"></img>\n",
    "\n",
    "Recap: \n",
    "* as you run beam search you see a lot of sentences with \n",
    "    * length equal 1, \n",
    "    * length sentences were equal 2,\n",
    "    * up to 30, if we consider to output sentences up to 30 words\n",
    "* You would be keeping track of the top three possibilities for each of these possible sentence length 1, 2, 3, 4, and so on up to 30\n",
    "* I'll put all those sentences and score them against the score (last sum on the image above):  you can take your top sentences and just computes this objective function on the sentences that you have seen\n",
    "* You pick the one with highest vlaue\n",
    "\n",
    "**Beam search discussion**\n",
    "\n",
    "Implementation details: how to choose B:\n",
    "* If B is large \n",
    "    * You consider a lot of different options => better result\n",
    "    * But search is slow\n",
    "* If B small: vice versa\n",
    "\n",
    "* In production B might be around 10. \n",
    "* 100 – very large to production system\n",
    "* For research papers: may be 1000\n",
    "\n",
    "Compared to exact search algorithms (Depth / Width first search) beam search runs much faster but is not guaranteed to find the exact maximum for this augments that you like to find.\n",
    "\n",
    "Beam Search is a widely used algorithm in many production and commercial systems\n",
    "\n",
    "<img src='./Images/W3_10.png' style=\"width: 60%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1ea34",
   "metadata": {},
   "source": [
    "### (video 5) Error Analysis in Beam Search\n",
    "\n",
    "You sometimes wonder, Is beam width working well enough? There are some simple things we can compute to give you guidance on whether you need to work on improving your search algorithm. \n",
    "\n",
    "* how error analysis interacts with beam search and how you can figure out \n",
    "    * whether it is the beam search algorithm that's causing problems \n",
    "    * Or whether it might be your RNN model that is causing problems\n",
    "* Notation\n",
    "    * Y* – good translation provided by the human\n",
    "    * Y_hat – bad translation provided by the algorithm\n",
    "\n",
    "2 components in the model\n",
    "* RNN: seq to seq\n",
    "* Beam search that analyses the output\n",
    "\n",
    "it's always tempting to \n",
    "* collect more training data that never hurts.\n",
    "* increase the beam width that never hurts or pretty much never hurts\n",
    "\n",
    "it turns out that the most useful thing for you to do at this point is to compute  \n",
    "* using this model P(y*|x) \n",
    "* and P(y_hat|x) using your RNN model\n",
    "* then to see which of these two is bigger\n",
    "    * Depending on which of these two cases hold true, you'd be able to more clearly ascribe this particular error to one of the RNN or the beam search algorithm being had greater fault\n",
    "    \n",
    "\n",
    "<img src='./Images/W3_11.png' style=\"width: 60%\"></img>\n",
    "\n",
    "Here is the logic behind this\n",
    "* Case 1: P(y*|x) > P(y_hat|x)\n",
    "    * The Beam search's job was to select most probable translation, and it didn't succeed\n",
    "* Case 2: P(y*|x) < P(y_hat|x)\n",
    "    * Y* is a better translation but according to RNN, P(y*|x) is smaller => RNN does bad job\n",
    "    * There's some subtleties pertaining to length normalizations that I'm glossing over.\n",
    "        * If you are using some sort of length normalization, \n",
    "        * instead of evaluating these probabilities, \n",
    "        * you should be evaluating the optimization objective that takes into account length normalization\n",
    "    \n",
    "<img src='./Images/W3_12.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**Error analysis process**\n",
    "\n",
    "*  You go through the development set and find the mistakes that the algorithm made in the development set\n",
    "* you can then carry out error analysis to figure out what fraction of errors are due to beam search versus the RNN model\n",
    "    * if you find that beam search is responsible for a lot of errors, then maybe is we're working hard to increase the beam width\n",
    "    * if you find that the RNN model is at fault, then you could do a deeper layer of analysis to try to figure out\n",
    "        * if you want to add regularization, \n",
    "        * or get more training data, \n",
    "        * or try a different network architecture, \n",
    "        * or something else\n",
    "\n",
    "<img src='./Images/W3_13.png' style=\"width: 60%\"></img>\n",
    "\n",
    "This particular error analysis process very useful whenever you have \n",
    "* an approximate optimization algorithm, such as beam search that is working to optimize some sort of objective,\n",
    "* and some sort of cost function that is output by a learning algorithm, such as a sequence-to-sequence model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ea0ff",
   "metadata": {},
   "source": [
    "### (video 6) Bleu\tscore (bilingual evaluation)\n",
    "\n",
    "* One of the challenges of machine translation: there could be multiple equally good translations\n",
    "* How do you evaluate a machine translation system in this case? (= how to measure accuracy?)\n",
    "* Convention: bleu score\n",
    "\n",
    "What the BLEU score does is \n",
    "* given a machine generated translation, \n",
    "* allows you to automatically compute a score that measures how good is that machine translation.  \n",
    "    * And the intuition is following: \n",
    "        * we're going to look at the machine generated output and \n",
    "        * see if the types of words it generates appear in at least one of the human generated references\n",
    "        *  these human generated references would be provided as part of the test set\n",
    "        \n",
    "<img src='./Images/W3_14.png' style=\"width: 60%\"></img>\n",
    "\n",
    "let's look at a somewhat extreme example:\n",
    "* Machine translatopn output \"the\" x N\n",
    "* Tne way to measure how good the machine translation output is, is:\n",
    "    * to look at each the words in the output and \n",
    "    * see if it appears in the references\n",
    "    * this would be called a **precision** of the machine translation output\n",
    "    *  **precision**  = what fraction of the words in the MT output also appear in the references\n",
    "        * there are seven words in the machine translation output. \n",
    "        * And every one of these 7 words appears in both references\n",
    "        * So this will have a precision of 7 over 7. It looks like it was a great precision\n",
    "* **Modified precision:** we will give each word credit only up to the maximum number of times it appears in the reference sentences\n",
    "    * [max number of times word appears in any of the references] / [number of times it appears in prediction]\n",
    "        * in Reference 1, the word, the, appears twice. \n",
    "        * In Reference 2, the word, the, appears just once. \n",
    "        * So 2 is bigger than 1, and so we're going to say that the word, the, gets credit up to twice.\n",
    "* In the BLEU score, you don't want to just look at isolated words. You maybe want to look at pairs of words as well\n",
    "    * Denominator = number of bigrams in translated sentences\n",
    "    * Numerator = clipped max of bigrams (max number appearing in at least one of the refreferences)\n",
    "\n",
    "<img src='./Images/W3_15.png' style=\"width: 60%\"></img>\n",
    "<img src='./Images/W3_16.png' style=\"width: 60%\"></img>\n",
    "\n",
    "* In final bluescore we can take uni, bi, tri, etc grams into account all together\n",
    "    *  if the MT output is exactly the same as either Reference 1 or Reference 2, then all of these values P1, and P2 and so on, they'll all be equal to 1.0.\n",
    "    \n",
    "**Combined Bleu score**\n",
    "* you sum p1, p2, p3, p4, \n",
    "* devide by 4, \n",
    "* and take exp of this\n",
    "* and adjust this (multiply) by BP factor = brevity penalty = \n",
    "    * if you input very short tranlsation it is easier to get hight precision\n",
    "    * BP penilizes short translations\n",
    "        * = 1 if MT_output_length > ref_output_length\n",
    "        * = exp(1 - ref_output_length/MT_output_length) otherwise\n",
    "\n",
    "<img src='./Images/W3_17.png' style=\"width: 60%\"></img>\n",
    "\n",
    "BLEU score is a useful single real number evaluation metric to use \n",
    "* whenever you want your algorithm to generate a piece of text\n",
    "* And you want to see whether it has similar meaning as a reference piece of text generated by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f9d83",
   "metadata": {},
   "source": [
    "### (video 7) Attention Model Intuition\n",
    "\n",
    "The Attention algorithm makes Encoder-Decoder models work better\n",
    "\n",
    "Classical Encoder-Decoder models\n",
    "* Input: very long sentence\n",
    "* Encoder: \n",
    "     * Endode it\n",
    "     * Memorize in activations\n",
    " * Decoder\n",
    "     * Generate translation\n",
    "  \n",
    "The way a human translator would translate this sentence is \n",
    "* not to first read the whole French sentence and then memorize the whole thing and then regurgitate an English sentence from scratch. \n",
    "* Instead, \n",
    "    * he/she would read the first part of it, \n",
    "    * maybe generate part of the translation. \n",
    "    * Look at the second part, generate a few more words, \n",
    "    * look at a few more words, generate a few more words and so on\n",
    "    * because it's just really difficult to memorize the whole long sentence like that\n",
    "* Encoder-Decoder architecture works quite well for short sentences, so we might achieve a relatively high Bleu score, but for very long sentences, maybe longer than 30 or 40 words, the performance comes down. It is difficult for NN to memorize long sentences as well\n",
    "* With attention models Bleu score doesn't drop down\n",
    "\n",
    "<img src='./Images/W3_18.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**Intuition**\n",
    "* Input: Jane visite lÁfrique en septembre\n",
    "* bi-directional RNN: to compute some set of features for each of input words\n",
    "    * Since it is not word-to-word translation we get rid of Y's\n",
    "    * But for each of words (positions in the sentence) we compute a rich set of features which take into account surrounding words, etc\n",
    "* We are going to use another RNN to generate english translation\n",
    "    * we denote activation of this RNN by S (not to confuse with A for first RNN) = hidden state\n",
    "    * We hope the first output would be Jane\n",
    "    * The question is, what part of French sentence should be taken into account to generate the first word?\n",
    "        * Attention model will be computing the set of attention weights\n",
    "        * alpha_1_1 – the weight which denotes how much attention should pay the first unit of the second RNN for the first word in input to correctly generate the first word of the output\n",
    "    * => together all alphas describe the context of the input we should take into account to generate first word of the output\n",
    "        * each alpha_t_t' \n",
    "            * t - the timestep of the output = the English word \n",
    "            * t' – the timestep of the input = the french word \n",
    "        * depends on forward and backward activations at timestep t' of the first RNN + state of the previous step of the second RNN (= s)\n",
    "       \n",
    "<img src='./Images/W3_19.png' style=\"width: 60%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59895b6c",
   "metadata": {},
   "source": [
    "### (video 8) Attention Model\n",
    "\n",
    "* to simplify the notation \n",
    "    * going forwards at every time step, even though you have the features computed from the forward occurrence and from the backward occurrence in the bidirectional RNN\n",
    "    * I'm just going to use a of t to represent both of these **concatenated** together \n",
    "    * a<sup>t'</sup> = concatenated (a_forward<sup>t'</sup>, a_backward<sup>t'</sup>)\n",
    "* Single direction RNN to generate translation\n",
    "    * Depends on \n",
    "        * s<sup>0</sup>\n",
    "        * context C which depends on alpha_1_t''\n",
    "    * context is actually be a way to sum the features from the different time steps (activations fron the first RNN) waited by these attention waits. So more formally the attention waits will satisfy \n",
    "        * they are all be non-negative, so it will be a zero positive and \n",
    "        * they'll sum to one\n",
    "    * alpha_t_t'is the amount of attention that's y_t should pay to a_t_prime.\n",
    "       \n",
    "<img src='./Images/W3_20.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**Computing attention alpha<sup>t,t'</sup>** \n",
    "* Compute exp(e<sup>t,t'</sup>)\n",
    "* Use softmax to essentially make sure that these waits sum to one if you sum over t'.\n",
    "* How to compute vectrs e<sup>t,t'</sup>? One way to do is to use a small NN\n",
    "    * Input\n",
    "        * s<sup>t-1</sup> (the previous state of second RNN)\n",
    "        * a<sup>t'</sup> (the previous state of second RNN)\n",
    "    * Intuition: if you want to decide how much attention to pay to the activation of t', it seems like it should depend \n",
    "        * the most on is what is your own hidden state activation from the previous time step\n",
    "        * and on each of the input words' features\n",
    "        * But we don't know what the function is. So we just train a very small neural network to learn whatever this function should be (backprop + gradient descent)\n",
    "\n",
    "<img src='./Images/W3_21.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**Downside** of this algorithm: quadratic cost to run\n",
    "* Tx input, Ty output => total number of attention params Tx x Ty\n",
    "* In ML applications for medium long sentences it is ok, \n",
    "* But doesn't work for research work\n",
    "\n",
    "**The summary:**\n",
    "* First recurrent RNN\n",
    "    * input: x<sup>t'</sup>\n",
    "    * activations a<sup>t'</sup>\n",
    "* Small regular NN computes vector e<sup>t,t'</sup> \n",
    "    * it depends on\n",
    "        * s<sup>t-1</sup> (the previous state of second RNN)\n",
    "        * a<sup>t'</sup> (the previous state of second RNN) \n",
    "    * and is used to calculate attention (alpha<sup>t,t'</sup>) y<sup>t</sup> should pay for a<sup>t'</sup> by applying softmax function\n",
    "* This attention is used to calculate context С<sup>t</sup> (summ all of attentions alpha<sup>t,t'</sup> over t')\n",
    "* Second regular RNN\n",
    "    * takes as input\n",
    "        * previous predicted word y_hat<sup>t-1</sup>\n",
    "        * previous state s<sup>t-1</sup>\n",
    "        * Context С<sup>t</sup> \n",
    "    * calculates new state s<sup>t</sup>\n",
    "    * outputs y_hat<sup>t</sup>\n",
    "    \n",
    "    \n",
    "The same approach works in image captioning\n",
    "\n",
    "In the exercise you get to implement the attention for the date normalization problem.\n",
    "\n",
    "We can also look the visualization of the attention weights: for corresponding input and output words you find that the attention waits will tend to be high\n",
    "\n",
    "<img src='./Images/W3_22.png' style=\"width: 60%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f61e94",
   "metadata": {},
   "source": [
    "# Speach recognition - Audio data\n",
    "\n",
    "### (video 9) Speech Recognition\n",
    "\n",
    "Problem: given an audio clip, x, and your job is to automatically find a text transcript, y\n",
    "* audio clip = air pressure against time\n",
    "* the human ear has physical structures that measures the amounts of intensity of different frequencies, \n",
    "    * => a common pre-processing step for audio data is to run your raw audio clip and generate a **spectrogram**\n",
    "        * horizontal axis is time, \n",
    "        * and the vertical axis is frequencies, \n",
    "        * and intensity of different colors shows the amount of energy \n",
    "    * = how loud is the sound at different frequencies at different times\n",
    "* once upon a time, speech recognition systems used to be built using **phonemes** = hand-engineered basic units of cells\n",
    "* With deep learning phonems representations became unnecessary\n",
    "    * instead, you can built systems that input an audio clip and directly output a transcript \n",
    "    * one of the things that made this possible was going to much larger data sets.\n",
    "        * Academic datasets for speach recoginition: 300-3000h\n",
    "        * Commercial datasets: 10k-100k hours        \n",
    "\n",
    "<img src='./Images/W3_23.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**How to build speach recognition system?**\n",
    "\n",
    "**First method: as discussed above**\n",
    "* You take different timeframes of an audio input \n",
    "* Build the attention model outputting the transcript\n",
    "\n",
    "<img src='./Images/W3_24.png' style=\"width: 60%\"></img>\n",
    "\n",
    "**Second method: CTC cost for speech recognition**\n",
    "Connectionist temporal classification\n",
    "* We're going to use a NN with an equal number of input x's and output y's,\n",
    "    * I draw a simple (uni-directional), but in practice, this will usually \n",
    "        * be a bidirectional LSTM \n",
    "        * or bidirectional GRU \n",
    "        * and usually, a deeper mode\n",
    "    * The number of timesteps is very big\n",
    "        *  if you have 10 seconds of audio \n",
    "        * and your features come at a 100 hertz \n",
    "        * so 100 samples per second, \n",
    "        * then a 10 second audio clip would end up with a thousand inputs\n",
    "* The CTC cost function allows the RNN to generate an output like this\n",
    "    * ttt_h_eee___ ___qqq__\n",
    "    * _ = blank character ≠ space character\n",
    "    * this is considered to be a correct output for \"the q\"\n",
    "* the basic rule for the CTC cost function is to collapse repeated characters not separated by \"blank\"\n",
    "    * phrase here \"the quick brown fox\" including spaces actually has 19 characters, \n",
    "    * and if somehow, the NN is forced upwards of a thousand characters by allowing the network to insert blanks and repeated characters and can still represent this 19 character \n",
    "\n",
    "<img src='./Images/W3_25.png' style=\"width: 60%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83d67d",
   "metadata": {},
   "source": [
    "### (video 10) Trigger Word Detection\n",
    "\n",
    "Easier to be done with smaller amount of data then needed to be used in speach recognition\n",
    "* Alexa\n",
    "* Siri\n",
    "* Google home\n",
    "\n",
    "Trigger the recognition by using a particular word. \n",
    "* This area is stil evolving, no wide consensus on what's the best way to trigger\n",
    "* Only one example is shown\n",
    "* Previously see:  \n",
    "    * RNN taking audio clip,\n",
    "    * compute spectrogram features. \n",
    "    * generates features, x1, x2, x3 audio features,\n",
    "    * pass them through an RNN\n",
    "* the remains to be done is to define the target labels\n",
    "    * you can set the target labels to be 0 for everything before the point the trigger word ends to be pronounced\n",
    "    * and right after that to set the target label of 1.\n",
    "\n",
    "<img src='./Images/W3_26.png' style=\"width: 60%\"></img>\n",
    "\n",
    "This could work reasonably well. But\n",
    "* One slight disadvantage of this is it creates a very imbalanced training set to have a lot more 0s than 1s.\n",
    "* one other thing you could do, this is a little bit of a hack, but could make the model a little bit easier \n",
    "    * train is instead of setting only a single time step output 1, \n",
    "    * you can actually make it output a few 1s for several times \n",
    "    * or for a fixed period of time before reverting back to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbacd0f",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "<img src='./Images/Q3_1.png' style=\"width: 80%\"></img>\n",
    "<img src='./Images/Q3_2.png' style=\"width: 80%\"></img>\n",
    "<img src='./Images/Q3_3.png' style=\"width: 80%\"></img>\n",
    "<img src='./Images/Q3_4.png' style=\"width: 80%\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
