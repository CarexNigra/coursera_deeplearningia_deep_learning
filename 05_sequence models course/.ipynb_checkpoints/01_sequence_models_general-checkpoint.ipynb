{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seqence models \n",
    "\n",
    "### (video 1) Sequence problems\n",
    "Examples of sequence data\n",
    "- speach recognition: x(audio = sequence) => y(transcript = sequence)\n",
    "- music generation: x(nan; single integer, referring to a style) => y(sequence)\n",
    "- sentiment classification: x(prase = sequence) => y(stars)\n",
    "- DNA sequence analysis: x(DNA code = sequence) => y (which part corresponds to a protein = sequence)\n",
    "- machine translation: x(phrase = sequence) => y(translation = sequence)\n",
    "- video activity recognition: x(videoframes = sequence) => y(activity)\n",
    "- NER: x(phrase = sequence) => y(entities)\n",
    "\n",
    "Problems can be addressed as supervised learning with labled data X, and Y as a training set\n",
    "=> but different types of sequence problems\n",
    "- X and Y are sequences\n",
    "- X and Y can have different length or same \n",
    "- only X or Y can be the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (video 2) Notations to define sequence problems\n",
    "NER: identify people\n",
    "\n",
    "\n",
    "|  | word | word | word | word | word | word | word | word | word |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| input x: | Harry | Potter | and | Hermione | Granger | invented | a | new | spell |\n",
    "| input features: | x<sup><1></sup> | x<sup><2></sup>  | ... | ... | x<sup>&lt;t&gt;</sup>  | ... | ... | x<sup><8></sup>  | x<sup><9></sup>  |\n",
    "| output y1: | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 |\n",
    "| output: | y<sup><1></sup> | y<sup><2></sup>  | ... | ... | y<sup>&lt;t&gt;</sup>  | ... | ... | y<sup><8></sup> | y<sup><9></sup>  |\n",
    "\n",
    "y1 – not the best representation, since doesn't tell you, where is the start and end of peoples' names\n",
    "\n",
    "the input is 9 words => we'll have 9 sets of features representing these\n",
    "- x<sup>&lt;t&gt;</sup> – feature in the middle of the sequence\n",
    "- *t* implies that these are temporal sequences (but it will be used regardless the type of sequence)\n",
    "- T<sub>x</sub> – length of the input sequence = 9\n",
    "- T<sub>y</sub> – length of the output sequence = 9\n",
    "- T<sub>x</sub> and T<sub>y</sub> can be different\n",
    "        \n",
    "- X<sup>(i)</sup> – i-th training example, in this case, this particular phrase\n",
    "- x<sup>(i)&lt;t&gt; </sup> – t-th element of an input sequence of i-th training example   \n",
    "- T<sub>x</sub><sup>(i)</sup> – the length of an input sequence of i-th training example  \n",
    "- y<sup>(i)&lt;t&gt; </sup> – t-th element of an output sequence of i-th training example \n",
    "- T<sub>y</sub><sup>(i)</sup> – the length of an output sequence of i-th training example\n",
    "    \n",
    "**Representation of individual words of the sentence**\n",
    "- You come up with a vocabulary = dictionary\n",
    "- Usually 30-50k words, around 100k – not uncommon as well\n",
    "    \n",
    "| words | number |\n",
    "| --- | --- |\n",
    "|a| 1 |\n",
    "|aaron| 2 |\n",
    "|...| ... |\n",
    "|and| 367 |\n",
    "|...| ... |\n",
    "|harry| 4075 |\n",
    "|...| ... |\n",
    "|potter| 6830 |\n",
    "|...| ... |\n",
    "|zulu| 10000 |\n",
    "\n",
    "- to build this dictionaly: one of ways: find top 10k occuring words\n",
    "- then use one hot representations to encode each of this words:\n",
    "    - x<sup><1></sup> = vetor [0, ... , 1, ...., 0] with unique 1 in 4075-th position\n",
    "\n",
    "And the *goal* is given this representation for X to learn a mapping using a sequence model to then target output y, I will do this as a supervised learning problem, I'm sure given the table data with both x and y.\n",
    "    \n",
    "If you encounter a word that is not in your vocabulary, you create a new token or a new fake word called *Unknown* = UNK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (video 3) Recurrent Neural Network (RNN) model\n",
    "\n",
    "**Why not standard network?**\n",
    "\n",
    "9 input words = [x<sup><1></sup>, ..., x<sup>&lt;t&gt;</sup>, ..., x<sup>&lt;T<sub>x</sub>&gt;</sup>]\n",
    "\n",
    "feed them into standard NN of a few layers => \n",
    "output of 9 values 0 or 1 = [y<sup><1></sup>, ..., y<sup>&lt;t&gt;</sup>, ..., y<sup>&lt;T<sub>y</sub>&gt;</sup>] \n",
    "    \n",
    "which gonna tell you whether each of those words are part of persons name\n",
    "\n",
    "But this not gonna work, cause:\n",
    "- Inputs and outputs can be of different lengths in different examples (and even no maximum sentence length is known)\n",
    "- Doesn't share feautures learnt across different positions of text: \"Harry\" in first position recognized as a name won't imply that \"Harry\" in some other position will be recognized as such\n",
    "\n",
    "Similarly to CNN \n",
    "- you want things learned for one part of the image to generalize quickly to other parts of the input (in CNN - image, in SM - sequence)\n",
    "- better representation will also let you reduce the number of parameters in your model\n",
    "\n",
    "If we think of reqular network: each of x<sup>&lt;t&gt;</sup> is a 10k onehot encoded vector => so the total input size would be 10k x max amount of words in phrase => the weight matrix would be too huge\n",
    "\n",
    "**What is RNN?**\n",
    "- We read sentence from left to right\n",
    "- We take first word x<sup>&lt;1&gt;</sup> and feed it to a first NN layer. We try to predict an output y_hat<sup>&lt;1&gt;</sup>: whether it is a part of person's name or not\n",
    "- RNN when it goes to read second word x<sup>&lt;2&gt;</sup>, \n",
    "    - instead of just predicting y_hat<sup>&lt;2&gt;</sup> using x<sup>&lt;2&gt;</sup>, \n",
    "    - it also gets some information from the comptuation of step 1\n",
    "    - in particular: an **activation** value from time step one is passed to time step 2: a<sup>&lt;1&gt;</sup>\n",
    "- and so on for each next word: input x<sup>&lt;t&gt;</sup> => y_hat<sup>&lt;t&gt;</sup>\n",
    "- till the last ont: input x<sup>&lt;T<sub>x</sub>&gt;</sup> => y_hat<sup>&lt;T<sub>y</sub>&gt;</sup>\n",
    "- In this example, T<sub>x</sub> = T<sub>y</sub>, but the architecture will change a bit if T<sub>x</sub> ≠ T<sub>y</sub>\n",
    "- At the begining we pass an activation a<sup>&lt;0&gt;</sup> which can be \n",
    "    - randomly initiated\n",
    "    - vector of zeros\n",
    "    \n",
    "Representation of RNN: \n",
    "\n",
    "x<sup>&lt;t&gt;</sup> => [ooooooooo] (loop + shaded box = time delay of one step) => y<sup>&lt;t&gt;</sup>\n",
    "\n",
    "**Parameters**\n",
    "- The recurrent neural network scans through the data from left to right. \n",
    "- The parameters it uses for each time step are shared  \n",
    "    - w<sub>ax</sub> = govern the connection from X1 to the hidden layer\n",
    "        - The second index means that this w<sub>ax</sub> is going to be multiplied by some X-like quantity\n",
    "        - \"a\" means that this is used to compute some a-like quantity (see equations below)\n",
    "    - w<sub>aa</sub> =  govern the horizontal connections = the activations\n",
    "    - w<sub>ya</sub> = govern the output predictions\n",
    "\n",
    "So, to make a prediction for y_hat<sup>&lt;3&gt;</sup>, this RNN gets the information not only from x<sup>&lt;3&gt;</sup> but also the information from x<sup>&lt;1&gt;</sup> and x<sup>&lt;2&gt;</sup> because the information on x<sup>&lt;1&gt;</sup> can pass horizontally\n",
    "\n",
    "one **weakness** of this RNN is that it only uses the information that is earlier in the sequence to make a prediction. This can be overcome in Bidirectional RNN\n",
    "\n",
    "**What are the calculations this RNN does?**\n",
    "- a<sup>&lt;0&gt;</sup> = [0] (vector)\n",
    "- a<sup>&lt;1&gt;</sup> = g(w<sub>aa</sub> * a<sup>&lt;0&gt;</sup> + w<sub>ax</sub> * x<sup>&lt;1&gt;</sup> + b<sub>a</sub>)\n",
    "    - g = activation function, \n",
    "        - often is \"tanh\" \n",
    "        - RelU (preventing the vanishing gradient problem)\n",
    "    - b<sub>a</sub> = bias    \n",
    "- y_hat<sup>&lt;1&gt;</sup> = f(w<sub>ya</sub> * a<sup>&lt;1&gt;</sup> + b<sub>y</sub>)\n",
    "    - f = activation function (same or another), depends on what the output is\n",
    "        - binary classification problem (NER) = sigmoid activation function\n",
    "        - k-way classification problem = softmax\n",
    "    - b<sub>y</sub> = bias\n",
    "    \n",
    "More generaly \n",
    "- a<sup>&lt;t&gt;</sup> = g(w<sub>aa</sub> * a<sup>&lt;t-1&gt;</sup> + w<sub>ax</sub> * x<sup>&lt;t&gt;</sup> + b<sub>a</sub>)\n",
    "- y_hat<sup>&lt;t&gt;</sup> = f(w<sub>ya</sub> * a<sup>&lt;t&gt;</sup> + b<sub>y</sub>)\n",
    "\n",
    "Simplification of these two equations\n",
    "- a<sup>&lt;t&gt;</sup> = g (w<sub>a</sub> * [a<sup>&lt;t-1&gt;</sup>, x<sup>&lt;t&gt;</sup>]+ b<sub>a</sub>)\n",
    "    - w<sub>a</sub> is a matrix which is defined as a horizontal stack of matricies [w<sub>aa</sub>; w<sub>ax</sub>]\n",
    "    - if dim(a) = 100 and dim(x) =10k => \n",
    "        - dim(w<sub>aa</sub>) = 100; 100\n",
    "        - dim(w<sub>ax</sub>) = 100; 10k\n",
    "        - dim(w<sub>a</sub>) = 100; 10100\n",
    "    - [a<sup>&lt;t-1&gt;</sup>, x<sup>&lt;t&gt;</sup>] is two vectors stacked together vertically \n",
    "        - dim(a<sup>&lt;t-1&gt;</sup>) = 100, 1\n",
    "        - dim(x<sup>&lt;t&gt;</sup>) = 10k, 1\n",
    "        - dim([a<sup>&lt;t-1&gt;</sup>, x<sup>&lt;t&gt;</sup>]) = 10100, 1\n",
    "    - The advantage of this notation is that rather than carrying around two parameter matrices, Waa and Wax, we can compress them into just one parameter matrix Wa\n",
    "- y_hat<sup>&lt;t&gt;</sup> = f(w<sub>y</sub> * a<sup>&lt;t&gt;</sup> + b<sub>y</sub>) (similarly)\n",
    "    - w<sub>y</sub> and b<sub>y</sub> denotes what type of output quantity we're computing.\n",
    "        - w<sub>y</sub> indicates a weight matrix or computing a y-like quantity, \n",
    "        - w<sub>a</sub> and b<sub>a</sub>  on top indicates that thes eparameters are for computing activation output quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (video 4) Back propagation through time\n",
    "\n",
    "**Element-wise (individual time steps) loss function**\n",
    "\n",
    "A certain word in the sequence is supposed to be a person's name: y<sup>&lt;t&gt;</sup> = 1\n",
    "And the NN outputs some probability of the particular word being a person's name: y_hat<sup>&lt;t&gt;</sup> = 0.1\n",
    "Loss = standard logistic regression loss, also called the cross entropy loss\n",
    "\n",
    "L<sup>&lt;t&gt;</sup>(y_hat<sup>&lt;t&gt;</sup>, y<sup>&lt;t&gt;</sup>) = - y<sup>&lt;t&gt;</sup> * log(y_hat<sup>&lt;t&gt;</sup>) - (1-y<sup>&lt;t&gt;</sup>) * log(1-y_hat<sup>&lt;t&gt;</sup>)\n",
    "\n",
    "**Overall loss**\n",
    "\n",
    "L(y_hat, y) = sum(t=1>T<sub>y</sub>) [L<sup>&lt;t&gt;</sup>(y_hat<sup>&lt;t&gt;</sup>, y<sup>&lt;t&gt;</sup>)]\n",
    "\n",
    "Backprop requires doing computations in the opposite directions. And that then, allows you to compute all the appropriate quantities that lets you then, take derivatives, respected parameters, and update the parameters using gradient descent\n",
    "\n",
    "In back propagation procedure, the most significant message (or the most significant recursive calculation) is the one between activations. And it gives this algorithm the name \"backpropagation through time\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
